---
title: "ML Basic"
author: "JEFFREY CHAN"
date: "12/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```      
# Linear Regression Essentials in R

load / install the requirement packages

```{r}
library(tidyverse)
library(caret)
theme_set(theme_bw())
```

Preparing the data

sample_n(data, sample data), will randomly pick # of sample data

```{r}
data("marketing", package = "datarium")
sample_n(marketing,3)
```

```{r}
# split the data into training and test set
set.seed(123)

# We’ll randomly split the data into training set (80% for building a
# predictive model) and test set (20% for evaluating the model).

training.samples <- marketing$sales %>%
  createDataPartition(p=0.8, list = F)
train.data <- marketing[training.samples, ]
test.data <- marketing[-training.samples, ]

# build model 
model <- lm(sales ~., data = train.data)

# summarize the model
summary(model)

summary(model)$coefficient

# make prediction 
predictions <- model %>% predict(test.data)

# model performance 
# (a) Prediction error, RMSE
RMSE(predictions, test.data$sales)

# (b) R-square
caret::R2(predictions, test.data$sales)
```

Simple Linear Regression 

```{r}
model_you <- lm(sales ~ youtube, data = train.data)
summary(model_you)$coef
```

Make prediction 
```{r}
newdata <- data.frame(youtube = c(0,1000))
model_you %>% predict(newdata)
```

Multiple Linear Regression 

```{r}
model3 <- lm(sales ~ youtube + facebook + newspaper, data = train.data)
summary(model3)$coef
```

Quick note, i have a lot of predictor, we can simply use ~. to include all the predictor

```{r}
model3_1 <- lm(sales ~., data = train.data)
summary(model3_1)$coef
```

Col1:  b0 / y intercept is 3.73546064
Col2: std.error = 0.44062. this represent the accuracy of the coefficients. We always want small value for std.error
Col3: T value is the t statistics estimate / std error = t value
Col4: P value for the T statistics. The smaller the p value the more 
significant the estimate is. 

Let's make prediction for the values 
```{r}
# New advertising budget
newdata <- data.frame( youtube = 2000, facebook = 1000, newspaper = 1000)

# predict sales values
model3_1 %>% predict(newdata)
```

Interpretation.

Before using the model for predictions, i need to access the statsitical significance of the model. We need to apply summary(model_name)
```{r}
summary(model)
```

The residuals, should be normally distributed. Looks like our data is kind of normally distributed. by theory, mean should be zero. Q1 and Q3, min and max should be around the same with + or - sign. 

coefficients, shows the parameter values and their significance. 
If they are significant, it will be shown with starts

RSE / R^2 / F statistics are used to check how well the model fits to our data

First step, always check the F statistics and the associated p value and the bottom of the model summary

Coefficients significance
```{r}
summary(model)$coef
```

Important, from the summary table , we can see that youtube, 
facebook advertising budget are significantly changing the sales while by newspaper, there is not much of a change. 

Also, to interpret the intercept, we can say every 1000 dollar i invest in facebook advertising i will have a return of 1000* 0.19398450 = 193 sale unit. 
So do youtube, 1000* 0.04516611 = 45.16 sale units 

Since newspaper does not affect the outcome much, lets remove it from the model 
```{r}
model4 <- lm(sales ~ youtube + facebook, data = train.data)
summary(model4)
```
now our equation can be written as 
sales = 3.577663 + 0.045287(youtube) + 0.190299(facebook)

Model accuracy

Well, after knowing it is significant, we would like to know how well the model fits our data. 
This process will be referred to as the goodness of fit
The overall quality of the linear regression fit can be shown by model summary
RSE, R^2, adjusted R^2, F statistics 

RSE / model sigma -> prediction error. it is the observed out come - predicted value (Y_i - Y bar). small rse is the best the model fits to the data.
dividing the RSE by the average value of the outcome variable will give me the prediction error rate. Which should be as small as possible. 

in our example, we have 1.853 for RSE. 
```{r}
(1.853/(mean(train.data$sales)))*100
```
It is 10.90% which is low. 

R-squared and adjusted R squared
R^2 is ranges between 0 and 1. Higher the R square the better the model. 
High R square = observed data is very close to the prediction data. So the quality of the regression line is pretty good.R^2 = pearson ^2. Here is the 
trick tho, if i have a lot of parameters, R^2 will increase along with it.

However, the adjusted R^2 is the correction of the for the number of 
parameters in the predictive model.Therefore, I should always read 
adjusted R^2 because it will make the correction according the to incorrect
R^2

When adjusted R^2 = 0 thats mean the model did not explain much about the variability in the outcome. 
In our outcome, adjusted R^2 is 0.9112 so it is pretty good. 

Lastly, F statistics gives the overall significance of the model. it tells us whether at least one predictor variables has non zero coefficient. 
In a simple linear regression ( one parameter), it wont be interesting
because it is just a duplicated info given by the t test from the coef table.

The F statistic becomes more important once we start using multiple 
predictors as in multiple linear regression.

So according to what we have, our F statistics equal 825.4 with 2 parameters 
and 159 df, with a p-value of 2.2e^-16. which is highly significant. In 
order to read the p value, p < 0.05 will be significant. 

Making predictions

Procedure to make predictions
1) predict the sales values based on new advertising budgets in the test data
2) Assess the model performance by computing:
  the prediction error RMSE (Root Mean Squared Error), representing the 
  average difference between the observed known outcome values in the test
  data and the predicted outcome values by the model. The lower the RMSE, the better the model. 
   The R^2, representing the correlation between the observed outcome values 
   and the predicted outcome values. the higher the r^2, the better the model. 
   
```{r}
# make predictions 
predictions <- model %>% predict(test.data)

# model performance 
# (a) compute the prediction error, RMSE
RMSE(predictions, test.data$sales)
```
```{r}
caret::R2(predictions, test.data$sales)
```

```{r}
(RMSE(predictions, test.data$sales)/ mean(test.data$sales))*100
```
The % error is 16.39% is alright

## Discussion

This section discuss basic linear regression and provides practical 
examples in R for computing simple and multiple linear regression model. 
Also, we have learnt how the accuracy of the model. 

The idea of linear regression is to see the predictor relationship with the response. 
It can be easily be visualized by a basic plot without working a lot on lm() summary and more. 

```{r}
ggplot(marketing, aes(youtube, sales)) + 
  geom_point() +
  stat_smooth()
```

As we can expected, since we know that the Beta_1 is positive from what we have done before, we can expect that the slope is positive and the scatter dots are following the positive slope.

Interaction Effect in Multiple Regression: Essentials

Considering our example, the additive model assumes that, the effect on
sales of youtube advertising is independent of the effect of facebook advertising.

This assumption might not be true. For example, spending money on facebook advertising may increase the effectiveness of youtube advertising on sales.
In marketing, this is known as a synergy effect, and in statistics it 
is referred to as an interaction effect (James et al. 2014).

Interaction effects


```{r}
# build the model
# use this:
model12 <- lm(sales ~ youtube*facebook, data = train.data)
summary(model12)
```

Make predictions 

```{r}
predictions12 <- model12 %>% predict(test.data)
# model performance
# (a) prediction error, RMSE
RMSE(predictions12, test.data$sales)
# (b) R-square
caret::R2(predictions12, test.data$sales)
# % error
(RMSE(predictions12, test.data$sales) / mean(test.data$sales))*100
```

Those values are pretty good. high R^2 and 10.67% error

```{r}
summary(model12)$coef
```
As you can see that all of them are significant (Pr(>|t|)). so it means 
there is an interaction relationship between the two predictor
variables (youtube and facebook advertising)

our model will be like 
sales = 7.89 + 0.0189949052(youtube) + 0.0330506939(facebook) + 0.0008751096 (youtube*facebook)

comparing the additive and the interaction models. 

The prediction error RMESE of the interaction model is 1.721177 compared
with the prediction error of the addictive model  2.642146, interaction
model is lower.

R^2 of the interaction model is 0.9373706, and for the addictive model has 0.8322611.
Interaction model has a better R^2

Lastly, these result suggest that the model with the interaction term is better than the model that contains only main effects. 

So for this specific data, we should go for the model with the
interaction model.

Discussion, after finding addictive model which is significant, we should also check if the interaction model is also significant. If they do, we should adapt the interaction model. 

Regression with Categorical Variables: Dummy Coding Essentials in R

```{r}
library(car)
# load data
data("Salaries", package = "car")
# Inspect the data
sample_n(Salaries, 3)
```

Categorical variables with two levels
From our data set, we would like to investigate differences in 
salaries between males and females. 
Based on the gender, we can say m = 1, female = 0
b0 + b1 if person is male
bo if person is female.
B0 is average salary among females
B0 + B1 = average salary among males
B1 is average difference in salary between males and females

```{r}
mwage <- lm(salary ~ sex, data = Salaries)
summary(mwage)$coef
```

From the above result, average salary for female is 101002.41.
Male = B0 + B1 = 101002.41 + 14088.01 = 115090.40

the P value for both sex is very significant, which is suggesting that there 
is a statistical evidence of a difference in average salary between
the genders.

```{r}
contrasts(Salaries$sex)
```

I can use the function relevel() to set the baseline category to males
as follow
```{r}
Salaries <- Salaries %>%
  mutate(sex = relevel(sex, ref = "Male"))
```


```{r}
mwage1 <- lm(salary ~ sex , data = Salaries)
summary(mwage1)$coef
```

## Categorical variables with more than two levels

Generally, categorical variable with n levels will be transformed in to n-1 variables each with two levels. These n-1 new variables contain the same 
info than the single variable. This recording creates a table called 
contrast matrix.

In salaries, data has three levels, asstprof, assocprof, and prof. These variables could be dummy coded in to two variables, one called assocprof and one prof. 

if rank = assocprof, then the column assocprof would be coded with 1 and prof with 0
if rank = prof, then the col assocprof would be coded with a 0 and prof would be coded with a 1
if rank = asstprof then both cols assocprof and prof would be coded with a 0

```{r}
res <- model.matrix(~rank, data=Salaries)
head(res[,-1])
head(res)
```
R will always use the first level as a reference and interpret the remaining levels relative to the first level

The following code will give you the level
```{r}
levels(Salaries$rank)
```
Indeed, AsstProf will be the reference level.

Also ANOVA(analysis of variance) is just a special case of linear model where 
the predictors are categorical variables. R understands the fact that ANOVA and regression are both examples of linear models, it lets you extract the classic ANOVA table from the regression model using the R base anova() function or the Anova() function. We generally use Anova() function because it automatically takes care of unbalanced designs.

Lets predict the salary from using a multiple regression procedure

```{r}
mwage2 <- lm(salary ~ yrs.service + rank + discipline + sex, data = Salaries)
Anova(mwage2)
```

```{r}
Anova(mwage1)
```

When we take rank, discipline, yrs of service in to consideration, the variable sex is no longer significant combined with the variation in salary between individuals.

```{r}
summary(mwage2)
```

```{r}
levels(Salaries$discipline)
```
apply ??salaries and check discipline a and b is corresponding to what field
from the documentation, dis A is theoretical, dis B is applied department. 

For example, from discipline B (applied departments) is significantly associated with an average increase of 13473.38 in salary compared (there is a difference) to discipline theoretical departments.
So that is the reason why discipline B is significant


Nonlinear Regression Essentials in R: Polynomial and Spline Regression Models

<http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/>

In this section, you’ll learn how to compute non-linear regression models 
and how to compare the different models in order to choose the one that fits the best your data.

Will also be using RMSE and R2 metric to compare the different models' accuracy
Recall, RMSE -> model prediction error. Thats the average difference of the observed outcome values and predicted outcome values.

R2 represents the squared correlation. How accurate is the data, if it is 
exactly on the regression line, that the R^2 will be 1

The best model is the model with the lowest RMSE and the highest R2

```{r}
library(tidyverse)
library(caret)
theme_set(theme_classic())
```

Prepare the data 

we will use the boston data from MASS package
```{r}
# Load the data, if the package is not installed, instal it now and load the library
if(!require("MASS")){
  install.packages("MASS")
  library(MASS)
}
data("Boston", package = "MASS")
str(Boston)
# Split the data in to training and test set
set.seed(123)
ts <- Boston$medv %>%
  createDataPartition(p =0.8, list = F)
trd <- Boston[ts, ]
ted <- Boston[-ts, ]
```
Visualize the scatter plot of the medv vs lstat varaibles, both medv and
lstat is from the Boston dataset

use stat_smooth when i want to display the results with non standard geom
```{r}
ggplot(trd, aes(lstat,medv)) + 
  geom_point() +
  stat_smooth()
```

It is obvious to see that the blue line is a curve


## Linear Regression

```{r}
# Build the model
modelc <- lm(medv ~ lstat, data = trd)
# make prediction
pmodelc <- modelc %>% predict(ted)
data.frame(
  rmseL <- RMSE(pmodelc, ted$medv),
  r2L <- caret::R2(pmodelc, ted$medv),
  perrorL <- rmseL / mean(ted$medv)
)
```
I have a pretty high percent error 26.82%

visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = lm, formula = y ~ x)
```

## Polynomial Regression

Lets make it polynomial regression
medv is the response.

we should have this following formula medv = b0 + b1 * lstat + b2*lstat^2
in r to make that ^2 we need to apply I(x^2)

```{r}
modelc1 <- lm(medv ~ lstat + I(lstat^2), data = trd)
summary(modelc1)
```

Alternative way to create 2 degree regression model

```{r}
modelc2<- lm(medv ~ poly(lstat, 2, raw =T), data = trd)
summary(modelc2)
modelc2p <- modelc2 %>% predict(ted)
p2 <- c(RMSE(modelc2p,ted$medv), caret::R2(modelc2p, ted$medv),
        RMSE(modelc2p, ted$medv) / mean(ted$medv))
```

As you can see they are the same. intercepts and the coefficient of beta1 and beta2^2 are all significant. as well as the F statistic P value is also small. Indeed, we have an Adjusted R^2 0.6329 which is ok. 

The following is the 6th order

```{r}
modelc6 <- lm(medv ~ poly(lstat, 6, raw = T), data = trd)
summary(modelc6)
modelc6p <- modelc6 %>% predict(ted)
p6 <- c(RMSE(modelc6p,ted$medv), caret::R2(modelc6p, ted$medv),
        RMSE(modelc6p, ted$medv) / mean(ted$medv))
```
From this point we can see that after degree 3 it is no longer significant. 
Then we will just simply use degree 3 for our model

```{r}
modelc5 <- lm(medv ~ poly(lstat, 5, raw = T), data = trd)
summary(modelc5)
```
```{r}
modelc5p <- modelc5 %>% predict(ted)
p5 <- c(RMSE(modelc5p,ted$medv), caret::R2(modelc5p, ted$medv), RMSE(modelc5p, ted$medv) / mean(ted$medv))
modelc3 <- lm(medv ~ poly(lstat, 3, raw = T), data = trd)
modelc3p <- modelc3 %>% predict(ted)
p3 <- c(RMSE(modelc3p,ted$medv), caret::R2(modelc3p, ted$medv), RMSE(modelc3p, ted$medv) / mean(ted$medv))
modelc4 <- lm(medv ~ poly(lstat, 4, raw = T), data = trd)
modelc4p <- modelc4 %>% predict(ted)
p4 <- c(RMSE(modelc4p,ted$medv), caret::R2(modelc4p, ted$medv), RMSE(modelc4p, ted$medv) / mean(ted$medv))
perrorp4 <- RMSE(modelc4p, ted$medv) / mean(ted$medv)
modelc7 <- lm(medv ~ poly(lstat, 7, raw = T), data = trd)
modelc7p <- modelc7 %>% predict(ted)
p7 <- c(RMSE(modelc7p,ted$medv), caret::R2(modelc7p, ted$medv), RMSE(modelc7p, ted$medv) / mean(ted$medv))
```

Lets see the RMSE, R2 and percent error p2, p3,p4, p5, p6

```{r}
temp <- cbind(p2,p3,p4,p5,p6,p7)
row.names(temp) <-c("RMSE", "R2", "% error")
temp
```

There are couple patterns are going on. First im looking at the table above, 
we can see that p5 is the dip of the % error. after that it bounces back and
has a small changes at p7. Also, if we look at the anova(p1 : p7) the significance level stops at 4.

```{r}
anova(modelc2,modelc3,modelc4,modelc5, modelc6, modelc7)
```

So i would say degree 4 is the best option.

```{r}
modelc4 <- lm(medv ~ poly(lstat, 4, raw = T), data = trd)
modelc4p <- modelc4 %>% predict(ted)
p4 <- c(RMSE(modelc4p,ted$medv), caret::R2(modelc4p, ted$medv), RMSE(modelc4p, ted$medv) / mean(ted$medv))
p4
```

```{r}
summary(modelc4)
```
The residuals distribution is alright. By theory, the median should be 0 
and 1Q, 3Q, and min max should be balanced. so this is not the best model but thats all we have. 

Let's visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ poly(x, 4, raw = T))
```

## Log Transformation
When i have a non linear relationship, i can also try a log transformation 
of the predictor variables. 

```{r}
# build the model 
modelcLog <- lm(medv ~ log(lstat), data = trd)

# make predictions
pmodelcLog <- modelcLog %>% predict(ted)

# model performance 
(rmseLog <- RMSE(pmodelcLog, ted$medv))
(r2Log <-  caret::R2(pmodelcLog, ted$medv))
(perrorLog <- rmseLog/ mean(ted$medv))
```

visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ log(x))
```

## Spline regression
```{r}
library(splines)
# splines becomes a base package so you shouldnt be install 
# it if you are using R version 4.0.2
knots <- quantile(trd$lstat, p = c(.25,.5,.75))
# build model 
modelcSp <- lm(medv ~ bs(lstat, knots = knots), data = trd)
# make prediction
pmodelcSp <- modelcSp %>% predict(ted)

# model performance
(rmseSp <- RMSE(pmodelcSp, ted$medv))
(r2Sp <-  caret::R2(pmodelcSp, ted$medv))
(perrorSp <- rmseSp/ mean(ted$medv))
```

Lets visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df =4))
```

## Generalized additive models

Here is another problem. Once I know that the model is non linear 
relationship in my data, the polynomial terms might not be flexible enough to capture the relationship, and the spline terms require specifying the knots. 
Therefore, we have Generalized additive models, GAM, are a technique to automatically fit a spline regression. We need mgcv package
```{r}
library(mgcv)
# build the model
modelcGam <- gam(medv ~s(lstat), data = trd)
# make predictions
pmodelcGam <- modelcGam %>% predict(ted)
# Model performance
data.frame(
  rmsegam <- RMSE(pmodelcGam, ted$medv),
  r2gam <- caret::R2(pmodelcGam, ted$medv),
  perrorGam <- rmsegam / mean(ted$medv))
```

Visualize the plot
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
```
Lets compare all the models linear, p^4, log, splines, and GAM
```{r}
(fourmodelsPercentError <- rbind(perrorL, perrorp4, perrorLog, perrorSp, perrorGam))
```
Among all 5 models, the best model we have is splines. why? think of how it works. 
it's like integral, it breaks in to small little part and find the slope thats why the stat_smooth fits the best of the data. Therefore Sp is the best model 

Multiple Linear Regression in R

A little bit duplicated with the first section, however, in this section
we are dealing with more than one variables.

With three predictor variables (x), the prediction of y is expressed by the following equation:

y = b0 + b1*x1 + b2*x2 + b3*x3

Outcome:
1) build and interpret a multiple linear regression model in R
2) Check the overall quality of the model 

```{r}
data("marketing", package = "datarium")
modelLS <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(modelLS)

```

Let's interpret the data

First we might want to look at the F statistics' p value
We have a pretty high significant value. This mean that, at least one of the variable is significantly related to the outcome. 
You can think of it is true that the parameter (youtube / facebook / newspaper) has the relationship with the response variable 

You should have this question, so which one has the relationship with the response variable?

let's take a look with the following code
```{r}
summary(modelLS)$coef
```

Look at the T statistics is checking if the b0, b1, b2, b3 is 0 or not. 
Look at the newspaper, we have -0.001037493 as the beta3 right? and look 
at the t value, -0.1767146. 

The T value for the newspaper is pretty close to 0 and look at B3, we have -0.001037493, it simply no use at all so we can simply create another model without the news paper. 

But the rest, youtube, facebook have a high impact for the sales. There is a relationship with the sales. So more money to put in to youtube and facebook,
we have more feedback from the sales. 

Lets create another model 

```{r}
modelLSyf <- lm(sales ~ youtube + facebook, data = marketing)
summary(modelLSyf)
```

95% confident interval for the coefficient

```{r}
confint(modelLSyf)
```

## Model accuracy assessment

As we know, simple linear regression, the overall quality is based on R^2 and Residual standard error (RSE)

Remember one thing, the more parameters we have the higher the R^2. 
Eventhough the parameter does not have much effect on the Y response, it will still in play with the R square.  Thats the reason why we have adjusted R^2

Residual standard error (RSE), or sigma

The RSE estimate gives a measure of error of prediction. we want small RSE
```{r}
sigma(modelLSyf)/mean(marketing$sales)
```
This number is not to bad. so we have approximately 12% error rate. 

Some cool trick, we dont have to type all the parameters. we can simply do 

model <- lm(sales ~. data = marketing) 
that ~ means all 

now if we would like to drop the newspaper from the parameter list
model <- lm(sales ~. - newspaper, data = marketing)

Alternative 
model <- update(model, ~. - newspaper)

Predict in R: Model Predictions and Confidence Intervals

Outcome: predict outcome for new observations data, display confidence 
intervals and the  prediction intervals. 

```{r}
# load the data 
data("cars", package = "datasets")
# build the model 
modelcar <- lm(dist ~ speed, data = cars)
modelcar 
```

## Prediction for new data set

```{r}
# create data 
cardata <- data.frame(speed <- c(12,19,24))
# predict data
predict(modelcar, newdata = cardata)
```

##Confidence interval 

by default the confidence interval is 95%

```{r}
predict(modelcar, newdata = cardata, interval = "confidence")
```

From our newly created data. Lets take 19 miles per hr as our example. 
Distance is in ft
we can say that 19 miles per hr has an average stopping dist between 51.82913 to 62.44421. the regression line has predict the value for 19mph with a stopping dist @ 57.13667

Prediction interval
prediction interval gives an uncertainty around a single value. 

```{r}
predict(modelcar, newdata = cardata, interval = "prediction")
```

From the table above, we can say that 95% prediction intervals tell us that with a speed of 19 mph with the stopping distance is 25.76 to 88.51. 
This means that, based on our model 95% of the cars with a speed of 19 mph have a stopping distance between 25.76 and 88.51

Now you should have at least one question in your mind at this point.
WTH? what is the difference between prediction interval and confidence interval?

The key word for both interpreting is average. 

prediction interval is predicting a single future value at that point. 

confidence interval is predicting the mean. 

<https://stats.stackexchange.com/questions/16493/difference-between-confidence-intervals-and-prediction-intervals>

More on prediction interval or confidence interval 

A prediction interval reflects the uncertainty around a single value, while a confidence interval reflects the uncertainty around the mean prediction values. Thus, a prediction interval will be generally much wider than a confidence interval for the same value.

Generally, we are interested in specific individual predictions. So a prediction interval would be more appropriate. Using a confidence interval when you should be using a prediction interval will be underestimate the uncertainty in a given predicted value 

lets build the prediction band and confidence inteval

```{r}
# build the model
data("cars", package = "datasets")
modelcar1 <- lm(dist~speed, data =cars)
# add the predictions
p.int <- predict(modelcar1, interval = "prediction")
carD <- cbind(cars, p.int)
# Regression line + confidence intervals
p <- ggplot(carD, aes(speed, dist)) + 
  geom_point() + 
  stat_smooth(method = lm) + 
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") + #prediction interval
  geom_line(aes(y = upr), color = "red", linetype = "dashed") # prediction interval 
p
```

# Section 2 - Regression Model Diagnostics - 3 Sub Sections

Intro
This section is focusing in making diagnostics to detect potential problems in the data. 

# Sub-Section 1  Linear Regression Assumptions and Diagnostics in R: Essentials

By the title, you can tell we are going to check if the model works well 
for the data.

1) inspect the significance of the regression beta coefficients
2) R^2

Those are what we have learnt from the previous sections

This section, we will learn the additional steps to evaluate how well the 
model fits the data.

For example, linear regression model makes the assumption that the 
relationship between x and y are linear. but that might not be true. The relationship could be polynomial or logarithmic 

the data might contain outlines which will affect the result. 

Thats why we need to diagnose the regression model and detect potential
problems and check the assumptions is met by the linear regression model 

in order to do so, we will check the distribution of residuals errors, 
which will tell us more about my data. 

Main idea for this section. 
 - explaining residuals errors and fitted values. 
 - present linear regression assumptions, as well as potential problems you 
 can will tackles while performing regression analysis
 - We will talk about some built in diagnostic plots in R to test the
 assumptions about our linear regression model. 
 
Lets get started 
 
```{r}
if(!require("broom")){
  install.packages("broom")
  library(tidyverse)
library(broom)
}
theme_set(theme_classic())
```


```{r}
# load the data 
data("marketing", package = "datarium")
# inspect the data
sample_n(marketing, 3)
```


Build the regression model 

```{r}
modelm <- lm(sales ~ youtube, data = marketing)
modelm
```

Fitted value ( predicted value) is the value that is predicted from the regression line. 
```{r}
ggplot(marketing, aes(youtube, sales)) +
  geom_point() +
  stat_smooth(method = lm)
```

augment is from broom package
```{r}
model.diag.metrics <- augment(modelm)
head(model.diag.metrics)
```
Lets plot the residuals error in red
```{r}
ggplot(model.diag.metrics, aes(youtube, sales)) + 
  geom_point() + 
  stat_smooth(method = lm, se = F) +
  geom_segment(aes(xend = youtube, yend = .fitted), color = "red", size =0.3)
```

In order to check the regression assumption, we need to see the
distribution of the residuals

Linear regression makes several assumptions about the data, such as :

Linearity of the data. The relationship between the predictor (x) and 
the outcome (y) is assumed to be linear.

Normality of residuals. The residual errors are assumed to be normally distributed.

Homogeneity of residuals variance. The residuals are assumed to have a
constant variance (homoscedasticity)

Independence of residuals error terms.


You should check whether or not these assumptions hold true. Potential 
problems include:

Non-linearity of the outcome - predictor relationships
Heteroscedasticity: Non-constant variance of error terms.
Presence of influential values in the data that can be:
  Outliers: extreme values in the outcome (y) variable
  High-leverage points: extreme values in the predictors (x) variable

Those problems can be solved by diagnostic plot

```{r}
par(mfrow = c(2,2))
plot(modelm)
```
How to read it?

<https://data.library.virginia.edu/diagnostic-plots/>

Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.

Normal Q-Q. Used to examine whether the residuals are normally distributed. 
It’s good if residuals points follow the straight dashed line.

Scale-Location (or Spread-Location). Used to check the homogeneity of
variance of the residuals (homoscedasticity). Horizontal line with equally
spread points is a good indication of homoscedasticity. This is not the case
in our example, where we have a heteroscedasticity problem.

Residuals vs Leverage. Used to identify influential cases, that is extreme
values that might influence the regression results when included or excluded
from the analysis. This plot will be described further in the next sections.

Lets create a better table for model.diag.metrics
```{r}
names(model.diag.metrics)
```
Before and after

```{r}
index <- c(1:nrow(model.diag.metrics))
model.diag.metrics1 <- data.frame(index,model.diag.metrics[ , c(-7)])
names(model.diag.metrics1)
```

fitted values = predicted value from the regression line
.resid = residual errors
.hat = outliers
.std.resid = detect outliers, extreme values -> standardized residuals = residuals / standard errors
.cooksd = Cook's distance -> detect influential values outliers or high 
leverage point


Let's look at the plot one by one
```{r}
plot(modelm,1)
```

Good plot would be show no fitted pattern, which means the red line should be a horizontal line at 0. If the red line is not at 0 or around 0 there might be a problem with our linear model, which means we need to use non linear model. 
In our example, plot 1 residuals vs fitted, there is no pattern in the residual plot, so we can assume it is a linear relationship sales ~ youtube.

if the residual plot is non linear relationship, you need to transform your data ith log(x), sqrt(x), x^2 in the regression and so on. 


You might ask how do you know if it is non linear? well it will be obvious that red line is something else like quadratic or something. 
reference link: non linear 
https://i0.wp.com/blogs.sas.com/content/iml/files/2019/06/residsmooth1.png?ssl=1


Homogeneity of variance

This assumption can be checked by examining the scale - location plot, 
also known as the spread- location plot
```{r}
plot(modelm,3)
```
Good plot for scale - location is the data set is close together and close the the line. 
since this plot is spreading wider when x increases, so this is not a good plot. 

So i can say not a constant variances in the residuals error (heterosedasticity)

Remember, by theory epsilon has mean zero and the variance is constant.

a possible solution to reduce the variance is to use log or square root transformation for variable y


lets try again by transforming the plot
```{r}
modelmLog <- lm(log(sales) ~ youtube, data = marketing)
plot(modelmLog,3)
```
This plot looks a lot better

Normality of Residuals
this is also called QQ plot. This plot can check the normality assumption. 
The normal probability plot of residuals should approximately follow a
straight line like below. 

```{r}
plot(modelm, 2)
```


Outliers and high Leverage points

The outliers will directly affect the RSE because it is so far away from the regression line. 
Outliers can be identified by examining the standardized residual (or 
studentized residual), which is the residual divided by the estimated standard error. 

Observations whose standardized residuals are greater than 3 in absolute value are possible outliers (James et al. 2014).

High leverage points (hat value)

A value of this statistic above 2(p + 1)/n indicates an observation with high leverage (P. Bruce and Bruce 2017); where, p is the number of predictors 
and n is the number of observations.

Residuals vs Leverage plot
```{r}
plot(modelm, 5)
```
The plot will highlight the most extreme points. 26, 179, 36. as you can see no outliers that have exceed 3 standard deviations, which is a good plot. 


Influential values 

Not all outliers (extreme data points) are influential in linear regression analysis

We use Cook's distance to determine the influence of a value. This metric
defines influence as a combination of leverage and residual size. 

High influence if Cook's distance exceeds 4/ (n - p - 1) n = # of observations 
p = number of predictor variables.

again residuals vs leverage plot can help us to find influential observations. outlying values are generally located at the upper right corner or lower right corner. 

Those corners will have direct influential against a regression line. 

```{r}
par(mfrow=c(1,2))
# Cook's distance
plot(modelm, 4)
# Residuals vs Leverage
plot(modelm, 5)
```
Now we have been talking about Cook's distance for the last 10 mins, so how do
I know if the outliers are influencing the regression line?

First from the Residuals vs Leverage plot, i am not seeing any red dashed line, so that means my outliers is not affecting the regression line a lot. If we see the dashed line that's mean we are close to the Cook's distance line which means it is some how affecting the regression line seriously. but usually if the outliers are within the Cook's distance we are all good. 

By default, only top 3 most extreme values are labeled on Cook's distance plot, if I want to add more i can use the following code
plot(modelm, 4, id.n = 5)

if i would like to access those distance later, i can use the following code
model.diag.metrics %?%
  top_n(3, wt = .cooksd)

Lets create something that is outside the Cook's distance

```{r}
dfcook <- data.frame(
  x <- c(marketing$youtube, 500,600),
  y <- c(marketing$sales, 80,100)
)
modelm2 <- lm(y~x, dfcook)
par(mfrow = c(1,2))
# Cook's distance
plot(modelm2, 4)
# Residuals vs Leverage
plot(modelm2, 5)
```
As you can see from the Residuals vs Leverage, you can see there are 2 outliers are outside of the dashed line (Cook's distance) those are the outliers that
are affecting the regression line directly. In the other words, those data are outside of the cook's distance would simply mean they have a high cook's value.

The plot identified the influential observation as #201 and #202. If you 
exclude these points from the analysis, the slope coefficient changes from 
0.06 to 0.04 and R2 from 0.5 to 0.6. Pretty big impact!

There are a lot of concepts that must be understood, so take your time to go
thru it again. 

## Discussion. 

This section describes linear regression assumptions and how to diagnose the potential problems in the model
You must visualizing the residuals and the patterns in residuals is not a stop signal. your current regression model might not be the best way to understand your data. 

Here are the potential problems:
non - linear relationships between the outcome and the predictor variables.
When facing to this problem, one solution is to include a quadratic term, 
such as polynomial terms or log transformation. See Chapter 

Existence of important variables that you left out from your model. Other variables you didn’t include (e.g., age or gender) may play an important
role in your model and data.

Presence of outliers. If you believe that an outliers have occurred due to an error in data collection and entry, then one solution is to simply remove the concerned observation.

# Sub-Section 2 - Multi-collinearity Essentials and VIF in R


In multiple regression, 2 or more  parameters might be correlated with each 
other is called collinearity. 

There is an extreme situation, called multicollinearity, which collinearity exists between three or more variables, even if no pair of variables have a particularly high correlation. 
To simplify that, there is redundancy between parameter variables. 

If we have multicollinearity in our model, it will become unstable. 

VIF (variance inflation factor) measures how much the variance of a regression coefficient is inflated due to multicollinearity.

"The smallest possible value of VIF is one (absence of multicollinearity). 
As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity (James et al. 2014)."

So in this section, we will be detecting the multicollinearity in a regression model using R

Required R packages
tidyverse, caret
```{r}
library("tidyverse")
library("caret")
```

Data Preparation

```{r}
# load the data 
data("boston", package = "MASS")

# split the data into training and test set
set.seed(123)
btr.samples <- Boston$medv %>% 
  createDataPartition(p=0.8, list = F)
btrd <- Boston[btr.samples, ]
bted <- Boston[-btr.samples,]
```

## Build a regression model

```{r}
# build the model
modelbos <- lm(medv ~., data = btrd)

# make predictions
bpred <- modelbos %>% predict(bted)

# model performance
brmse <- RMSE(bpred, bted$medv)
br2 <- caret::R2(bpred, bted$medv)
brmse
br2
```

Detecting multicollinearity
need to install car package for vif() 
```{r}
car::vif(modelbos)
```

From there we can see that tax has a 9.660128 score, which is a problem. 

Dealing with multicollinearity
```{r}
# build a model excluding the tax variable
modelbos.cleaned <- lm(medv ~. -tax, data = btrd)

# make predictions
modelbos.cleaned.pred <- modelbos.cleaned %>% predict(bted)

#  performance
brmse.cleaned <- RMSE(modelbos.cleaned.pred, bted$medv)
br2.cleaned <- caret::R2(modelbos.cleaned.pred, bted$medv)
brmse.cleaned
br2.cleaned
```
Now we have a lower R2 and a higher RMSE

This section is simply detecting and how to deal with multicollinearity in regression models. 
anything that is above 5 or 10, those parameters must be removed. 

Note that, in a large data set presenting multiple correlated predictor variables, you can perform principal component regression and partial least square regression strategies.

# Sub-Section 3 - Confounding Variable Essentials

library required gapminder
```{r}
library(gapminder)
lm(lifeExp ~ gdpPercap, data = gapminder)
```

So obviously the continent is an important variable like countries in Europe
are estimated to have a higher life expectancy compared to countries in Africa. So by adding continent as a confounding variable will increase the accuracy of our model.
```{r}
lm(lifeExp ~ gdpPercap + continent, data = gapminder)
```

# Regression Model Validation - 3 sections

Info 

In this big section, we will talk about goodness of the model, how well the
model fits the training data used to build the model and how accurate is the model in predicting the outcome for new unseen test observations. 

In this part, you will learn techniques for assessing regression model
accuracy and for validating the performance of the model. 

# Sub section 1 - Regression Model Accuracy Metrics: R-square, AIC, BIC, Cp and more

This section, we will be discussing different statistical regression metrics
for measuring the performance of the regression model 

Model Performance metrics

R-Squared: interval -> +/ - 1 +- 1 is the max or min. Closer to 1 the better
it is. It measures how close the actual data to the regression line. 
If we have 1 that means all the actual data are on exactly lies on the 
regression line. 

Root mean squared error RMSE: We want a small RMSE. RMSE is simply the square root of MSE. MSE is exactly the epsilon term from the regression model. 
Formula = MSE = actual - theoretical value) ^2 

Residual Standard Error (RSE): it is the same as model sigma. We want small RSE.
In real world practice, the difference between RMSE and RSE is very small, specifically when we are working on large scale multivariate data. 

Mean Absolute error (MAE): it is similar with RMSE. It's measures the
prediction error. 
formula = MAE = mean(abs(actual - theoretical))

Recall that, the more variable we add, R^2 will increase as well. So, we need another method to measure the accuracy of the model

Concerning R2, we have adjusted R- Squared, which adjusts the R2 for having too many variables.
Additionally, we have AIC, AICc, BIC, and Mallows Cp, that are commonly used 
for model evaluation and selection. Those are unbiased estimate of the model prediction error MSE. We want small # from those result. 

AIC (Akaike's information Criteria): this is developed by the Japanese Statistician. The idea of AIC is to penalize the inclusion of additional variables to a model. it adds a penalty that increase the error when including additional terms. The lower the AIC, the better the model. 

AICc: is a version of AIC corrected for a small sample sizes.

BIC (Bayesian information criteria): is derived from AIC with a stronger 
penalty for including additional variables to the model. 

Mallows Cp: is derived from AIC developed by Colin Mallows. 

Generally speaking, we use Adjusted R2, AIC, BIC, and Cp to measure the regression model quality and for comparing models.

Lets get in to the code

```{r}
if(!require("modelr")){
  install.packages("modelr")
  library(tidyverse)
  library(modelr)
  library(broom)
}

```
Example of data
```{r}
# load data
data("swiss")
# inspect the data
sample_n(swiss,3)

```

Build the regression models
model1 all parameters
model2 except examination
```{r}
models1 <- lm(Fertility ~., data = swiss)
models2 <- lm(Fertility ~., -Examination, data = swiss)
```

Now, lets check the model quality, the following code will look like duplicate
because im showing how to find those result in different ways. 

summary() will return Rsquared and adjusted R squared and the RSE
AIC() and BIC() computes the AIC and BIC respectively.

```{r}
summary(models1)
AIC(models1)
BIC(models1)
```

rsquare(), rmse() and mae() [modelr package], computes, respectively, the R2, RMSE and the MAE.
```{r}
models1.r2 <- rsquare(models1, data =swiss)
models1.rmse <- rmse(models1, data = swiss)
models1.mae <- mae(models1, data = swiss)
```

R2(), RMSE() and MAE() [caret package], computes, respectively, the R2, 
RMSE and the MAE.
```{r}
library(caret)
models1.pred <- models1 %>% predict(swiss)
models1.R2 <- caret::R2(models1.pred, swiss$Fertility)
models1.RMSE <- RMSE(models1.pred, swiss$Fertility)
models1.MAE <- MAE(models1.pred, swiss$Fertility)
```

glance() [broom package], computes the R2, adjusted R2, sigma (RSE), AIC, BIC.
```{r}
library(broom)
glance(models1)
```

Now lets compare the performance

```{r}
name <- c("adj.r.squared", "sigma", "AIC" , "BIC", "p.value")
models1.temp <- c(glance(models1))
models1.performance <- models1.temp[c(2,3,8,9,5)]
models2.temp <- c(glance(models2))
models2.performance <- models2.temp[c(2,3,8,9,5)]
models.performance <- rbind(models1.performance, models2.performance)                   
models.performance
```

From the above table we can see couple things

We have a better R^2 from model2.performance, which means models2 explain
better for the predicted value, the model from models2 has a better accuracy. 

RSE: models1 = 7.165369, models2 = 6.888644
RSE = sigma. So from models2 we have a smaller sigma which is a good thing. so models2 wins in this case.

Now, AIC and BIC. it is obvious that models2 has a better numbers than models1. remember we want small number from AIC and BIC. so models2 wins again

F statistics p value. 
```{r}
5.593799e-10 > 2.493363e-06
```
if you can compare it by looking at it, just calculate it with R. so models1 
has a smaller F statistics. However, imo, i would take models2.performance
as my model because of all those numbers compared with models1, models2 has
a better accuracy. 

```{r}
sigma(models1)/mean(swiss$Fertility)
sigma(models2)/mean(swiss$Fertility)
```
Lastly, as you can see, models2 has a better prediction error.

## Discussion

We have gone thru the overall performance of a regression model

The most important metrics are the Adjusted R^2, RMSE, AIC, and BIC.
These metrics are also used as the basis of model comparison and optimal model selection.

Note that regression metrics are all internal measures. They tell you how well the model fits to the data in hand, training data set. 

In general, we don't care how well the method works on the training data.
Rater, we are interested in the accuracy of the predictions that we obtain
when we apply our method to previously unseen test data. However, the test 
data is not always available making the test error very difficult to estimate. Because of this situation, we have cross-validation and bootstrap that are applied for estimating the test error ( the prediction error rate) using
training data.

# sub section 2 - Cross-Validation Essentials in R

intro\
Cross validation refers to a set of methods for measuring the performance of a given predictive model on new test data set\

Idea of Cross validation:\
1) Training set to build the model\
2) Testing set to see the prediction error\

Cross validation is also known as re-sampling method

In this section you will learn:\
1) most commonly used statistical metrics that measure the performance of a regression model in predicting the outcome of new test data\

2) validation set approach ( data split)\
   Leave one out cross validation\
   K - fold Cross validation\
   Repeated K fold cross validation \
   
   Those 4 methods have their advantages and drawbacks. Use it that fit the best of your problem. Mostly, K fold cross validation is recommended
   
required R packages
```{r}
library(tidyverse)
library(caret)
```
   
Preper the data
```{r}
# load the data
data("swiss")
# inspect the data 
sample_n(swiss,3)
```

To see the accuracy of the model on predicting the outcome. simply we want to estimate the prediction error.

1) R^2
2) RMSE: square root the MSE
3) compute the prediction errors

R^2, RMSE, MAE are used to measure the regression model performance during the cross-validation. 

Cross validation method
1) reserve a small sample of data set
2) build (or train) the model using the remaining part of the data set (unseen data)
3) test the effectiveness of the model on the reserved sample data set. If the model works well on the test data set, then it is good. (MSE)

The Validation set Approach

We split the data in to two part 80% for the training and 20% for the testing

```{r}
# split the data in to two part
set.seed(1234)
training.samples.swiss <- swiss$Fertility %>%
  createDataPartition(p =0.8, list =F)
train.data.swiss <- swiss[training.samples.swiss, ]
test.data.swiss <- swiss[-training.samples.swiss, ]
# build the model
model.swiss <- lm(Fertility ~., data = train.data.swiss)
# make predictions and compute the R2, RMSE, MAE
swiss.pred <- model.swiss %>%
  predict(test.data.swiss)
swiss.performance <- c(caret::R2(swiss.pred, test.data.swiss$Fertility),
                       RMSE(swiss.pred, test.data.swiss$Fertility),
                       MAE(swiss.pred, test.data.swiss$Fertility))
swiss.prediction.percent.error <- RMSE(swiss.pred, test.data.swiss$Fertility) / mean(test.data.swiss$Fertility)
swiss.performance
(swiss.prediction.percent.error*100)
```

When building the model, we always want to use the lowest test sample RMSE. 
RMSE and MAE are measured in the same scale as the outcome variable. 

Note that, the validation set method is only useful if you have a lot of data. Disadvantage is that, we build a model on a fraction of the data set only. We might have leaving out some interesting information about data, leading to 
higher bias. So the test error rate can be highly variable, depending on 
which observations are included in the training set and which observations
are included in the validation set. 


LOOCV (leave one out cross validation)

Process
1) leave  out one data point and build the model on the rest of the data 
2) Test the model against the data point that is left out at step 1 and record the test error associated with the prediction
3) repeat the process for all data points 
4) get the overall prediction error by taking the average of all these
test error estimates recorded at step 2

code: 

```{r}
# define training control
train.control <- trainControl(method = "LOOCV")

# Train the model
loocv.model <- train(Fertility ~., data = swiss, method = "lm", trControl = train.control)

# Summarize the result 
print(loocv.model)
```

Good thing about LOOCV is that we reduce the potential bias. However, the 
process is repeated a lot of time, so it will take a long time if we have a 
large data set. Because the model will be tested each data point at each iteration, which might yield a higher variation in the prediction error,
if some data points are outliers. So we need a good ratio of testing
data points, thats why we have k fold cross validation method. 

K Fold Cross Validation 

1) Randomly split the data set into k-subsets (or k-fold)(for example 5 subsets)
2) Reserve one subset and train the model on all other subsets
3) Test the model on the reserved subset and record the prediction error
4) Repeat this process until each of the k subsets has served as the test set.
5) Compute the average of the k recorded errors. This is called the cross-validation error serving as the performance metric for the model.

KFCV is a powerful method to estimate the accuracy of a model.

"The most obvious advantage of k-fold CV compared to LOOCV is computational. A less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV 
(James et al. 2014)."

the question will become, how to choose the right value of K

small k -> more biased and undesirable. high K = less biased but suffer 
from large variability. 

high value will lead us to LOOCV approach. small K will take use to
validation set approach

Most often, we use K= 5 / k = 10. These values have been shown empirically 
to yield test error rate estimates that suffer neither from excessively 
high bias nor form very high variance. 

The following we use k = 10 
```{r}
# define training control 
set.seed(321)
train.control.k <- trainControl(method = "cv", number = 10)

# train the model
k.model <- train(Fertility ~., data = swiss, method = "lm", trControl = train.control.k)

# summarize the results
print(k.model)
```

If you check both model, k.model has a better R squared MAE and RMSE.
```{r}
print(loocv.model)
```

Lets go for repeated K fold cross validation

```{r}
set.seed(432)
# Define training control
train.control.k.repeat <- trainControl(method = "repeatedcv", number = 10,
                                       repeats = 3)
# train the model
model.k.repeat <- train(Fertility ~., data = swiss, method = "lm", 
                        trControl = train.control)

# summarize the results 
print(model.k.repeat)
```
 Non repeat K fold 
 RMSE      Squared   MAE    
  7.462458  0.6749535  6.07299
if you compared non repeat K fold, the non repeat k fold is a better model because it has a higher R squared and lower RMSE MAE,


In this section, we have go thru 4 different methods for assessing the performance of a model on unseen test data. 
1) validation set approach, 2) leave-one-out cross-validation, 3) k-fold cross-validation 4) repeated k fold cross-validation

We generally recommend the repeated K fold cross-validation to estimate
the prediction error rate. It can be used in regression and classification settings.

another method is using bootstrap re-sampling methods, which consists of repeatedly and randomly selecting a sample of n observations from the original data set, and to evaluate the model performance on each copy. 

# Sub-Section 3 - Bootstrap Re-sampling Essentials in R

bootstrap re-sampling method can be used to measure the accuracy of a predictive model. Also, it can measure the uncertainty associated with any statistical estimator. 

Bootstrap re-sampling consist of repeatedly selecting a sample of n observations from the original data set and evaluate the model on each copy. An average standard error is then calculated with the results provide an indication of the overall variance of the model performance.

```{r}
library(tidyverse)
library(caret)
```

Load the data from swiss

```{r}
# load the data 
data("swiss")
# inspect the data
sample_n(swiss, 3)
```

## Bootstrap procedure

The bootstrap method is used to quantify the uncertainty associated with 
a given statistical estimator or with a predictive model.

It consists of randomly selecting a sample of n observations from the 
original data set. This subset, called bootstrap data set is then used to evaluate the model.

This procedure is repeated a large number of times and the standard error 
of the bootstrap estimate is then calculated. The results provide an 
indication of the variance of the models performance.

Note that, the sampling is performed with replacement, which means that 
the same observation can occur more than once in the bootstrap data set.

Evaluating a predictive model performance 

sample size 100
```{r}
# define training control 
train.control.bs <- trainControl(method = "boot", number = 100)

# train the model 
model.bs <- train(Fertility ~., data = swiss, method = "lm", trControl = train.control.bs)

# summarize the results 
print(model.bs)

```

the model is alright. not too bad RMSE and MAE, Rsquare is .609 is okay

Lets talk about quantifying an estimator uncertainty and confidence intervals 

Simply saying, we want to estimate the accuracy of the linear regression beta coefficient using bootstrap method. 
1) Create a simple function, model_coef(), that takes the swiss data set as 
well as the indices for the observations, and returns the regression coefficients.
2) apply the function boot_fun() to the full data set of 47 observations
in order to compute the coefficients

```{r}
model_coef <- function(data, index){
  coef(lm(Fertility ~., data = data, subset = index))
}
model_coef(swiss, 1:47)
```

Let use boot() function to compute the standard error of 500 bootstrap 
estimates for the coefficients 
```{r}
library(boot)
boot(swiss, model_coef, 500)
```

The original column corresponds to the regression coef, with the associated standard errors
t1 corresponds to the intercept, t2 corresponds to agriculture and so on
```{r}
names(swiss)
```
the standard error of the regression coefficient associated with Agriculture is 0.07. 
The standard errors measure the variability / accuracy of the beta coef. it can be used to compute the confidence intervals of the coefficients. 

```{r}
summary(lm(Fertility ~., data = swiss))$coef
```

The bootstrap approach does not rely on any of these assumptions made by the linear model, and so it is likely giving a  more accurate estimate of the coefficients and standard errors than the summary function. 



#Section 4 - Model Selection Essentials in R

Best subsets regression means test all possible combination of the predictors, and then select the best model. 

Stepwise regression means adding and deleting predictors in order to find the best performing model with a reduced set of variables.

Penalized Regression (ridge and lasso regression) and Principal components
based regression method. 

This this section we will talk about 
1) Best subsets selection 
2) Stepwise Selection
3) penalized Regression
4) Dimension Reduction Methods

load libraries and data
Quick guide, lapply will simply load all the libraries from the vector
```{r}
lib <- c("tidyverse", "caret", "leaps")
lapply(lib, require, character.only = T)
data("swiss")
sample_n(swiss, 3)
```

## Computing the best subset regression
nvmax = 5 is because we only have 5 variables
```{r}
modelsub <- regsubsets(Fertility ~., data = swiss, nvmax =5)
summary(modelsub)
```
Now, at the bottom of the table, row 1 to 5. It simply means the best 1, 2, ..., 5 variables model is what.

The best alone model is simply include education. Best 2 variables model is include education and catholic and so on for the next 3 models.

Now the problem is, which one to choose. Now, we need some statistical metrics 
or strategies to compare all the performance, and pick the best one, which
means low BIC, cp, and high Adj R2.

```{r}
perf.modelsub <- summary(modelsub)
df.performance.modelsub <- data.frame(
  Adj.R2 = which.max(perf.modelsub$adjr2),
  CP = which.min(perf.modelsub$cp),
  BIC = which.min(perf.modelsub$bic)
)
df.performance.modelsub
```

As we can see from the table above, Adj.R2 is the best with model 5, however; 
if we are looking at the CP and BIC, we should shift to model 4 instead. 

here is a important question, are those metrics from a test data or training data? 
You might need to sleep on it if you are having trouble to identify what is training data and testing data. 

Another approach is to select a models based on prediction error computed on a new test data using k fold cross validation techniques.

K fold cross-validation 

Remember, we either set k to 5 or 10.

Lets make a function
```{r}
#       id: model id
#   object: regsubsets object
#     data: data used to fit regsubsets
#  outcome: outcome varaible
get_model_formula <- function(id, object, outcome){
  # get model data
  getModelData <- summary(object)$which[id,-1]
  # get outcome varaible
  form <- as.formula(object$call[[2]])
  outcome <- all.vars(form)[1]
  # get model predictors
  predictors.a <- names(which(getModelData == T))
  predictors.a <- paste(predictors.a, collapse = "+")
  # build model formula 
  as.formula(paste0(outcome, "~", predictors.a))
}
```

lets get the best 3 variable model formula, 
```{r}
get_model_formula(3, modelsub, "Fertility")
```
let's build the get_cv_error() function for cross-validation error for a given model

```{r}
get_cv_error <- function(model.formula, data){
  set.seed(3)
  train.controlsub <- trainControl(method = "cv", number = 5)
  cv <- train(model.formula, data = data,
              method = "lm", trControl = train.control)
  cv$results$RMSE
}
```

lets get the error 
```{r}
# compute cross-validation error
model.ids <- 1:5
cv.errors <- map(model.ids, get_model_formula, modelsub, "Fertility") %>%
  map(get_cv_error, data = swiss) %>%
  unlist()
cv.errors
```

Now, get the smallest RMSE
```{r}
which.min(cv.errors)
```
```{r}
coef(modelsub, 4)
```

This entire section has gone thru how to find the best model out of multiple variables.

# Sub-Section 2 - Stepwise Regression Essentials in R

Stepwise Regression is constantly adding and removing predictor variables 
and see who has the best performance, which implies lowers prediction error.

There are three methods.

Forward selection, starts with no predictors and then stop when there is no longer statistically significant. 

Backward selection (or backward elimination), starts from all possible predictors, and removes the least contribute predictors and stop when all predictors are statistically significant. 

Stepwise selection (sequential replacement), which is a combination of forward and backward selections. start with no predictors, and sequentially adding the 
most contributing predictors (like forward selection). After adding each new variable, remove any variables that are no longer provide an improvement in
the model fit (like backward selection).

Restriction
Forward selection and stepwise selection can be applied in the high dimensional configuration, where the number of sample n is inferior to the number of predictors p, such as in genomic fields. 

Backward selection requires that the number of samples n is larger than the number of variables p, so that the full model can be fit. 

library tidyverse, caret, leaps, MASS
```{r}
libsteps <- c("tidyverse", "caret", "leaps", "MASS")
lapply(libsteps, require, character.only = T)
# fit the full model
full.model <- lm(Fertility ~., data = swiss)
# stepwise regression model
step.model <- stepAIC(full.model, direction = "both", trace = F)
```

regsubsets() from leaps package, which has the tuning parameter nvmax.
please see the previous section where i have initially introduced you this parameter. It returns multiple models with different size up to nvmax. 
you need to compare the performance of those models for picking the best one. regsubsets() has the option method for you to specify which selection you would like to make. 
backward, forward, seqrep
```{r}
modelselection <- regsubsets(Fertility ~., data = swiss, nvmax =5, method = "seqrep")
summary(modelselection)
```

train() from caret package also has a easier work flow to perform stepwise
selection using the leaps and mass package. It has an option named method, with the following values 
"leapBackward", "leapForward", "leapSeq"

Following code will also apply 10 k fold cv to estimate the average prediction error. 

```{r}
set.seed(999)
train.control.selection <- trainControl(method = "cv", number =10)
step.model.selection <- train(Fertility ~., data = swiss,
                              method = "leapBackward",
                              tuneGrid = data.frame(nvmax = 1:5),
                              trControl = train.control)
step.model.selection
```
Again, we want small # for RMSE and MAE, also we want as close to 1 as possible for r^2


```{r}
step.model.selection$bestTune
```
the bestTune wil give us the best model to select

```{r}
summary(step.model.selection$finalModel)
```

```{r}
coef(step.model.selection$finalModel, 4)
```

```{r}
step.model.selection.lm <- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, data = swiss)
ggplot(step.model.selection.lm, aes(x = Agriculture + Education + Catholic + Infant.Mortality, Fertility)) + 
  geom_point() +
  stat_smooth()
```

In MASS library, it has a function called stepAIC. you can use the direction parameter and set the value to both, forward, and backward for the selection.

In addition, caret package has method to compute stepwise regression using the MASS package, with method = "lmStepAIC"

please read the documentation on how to apply selection method.

## Conclusion, 

Stepwise regression is very useful and important for high - dimensional
data that has multiple predictor variables. Other alternatives are the
penalized regression (ridge and lasso regression) and the principal 
components based regression method (PCR and PLS)


# Sub-Section 3 - Penalized Regression Essentials: Ridge, Lasso & Elastic Net

The standard linear model doesn't work well when we have a lot of parameters. 
So we have alternative method which is penalized regression. This will add a constraint in the equation, which is known as shrinkage or regularization methods.

what the penalty does is to reduce the coefficient values towards zero. 
This will eliminate those less contribution variables to the regression model, down to zero or at least close to zero.

In the mean time, the reduction requires the tuning parameter (lambda) to determines the amount of shrinkage. 

In this section we will discuss the most commonly used penalized regression methods, including ridge regression, lasso regression and elastic 
net regression. 

Library
tidyverse, caret, glmnet (computing penalized regression)

```{r}
libpen <- c("tidyverse", "caret", "glmnet")
lapply(libpen, require, character.only = T)
data("Boston", package = "MASS")
# By now i hope you are already familiar wit the next 5 lines of code
set.seed(323)
training.samples.pen <- Boston$medv %>%
  createDataPartition(p = 0.8, list = F)
train.data.pen <- Boston[training.samples.pen, ]
test.data.pen <- Boston[-training.samples.pen, ]
# Predictor variables
x.pen <- model.matrix(medv~., train.data.pen)[, -1]
y.pen <- train.data.pen$medv
```

glmnet() is for computing penalized linear regression models.
glmnet(x.pen, y.pen, alpha =1, lambda = NULL)

x: matrix of predictor variables
y: the response or outcome variable, which is a binary variable.
alpha: the elasticnet mixing parameter. Allowed values include:
    “1”: for lasso regression
    “0”: for ridge regression
     a value between 0 and 1 (say 0.3) for elastic net regression.
lambda: a numeric value defining the amount of shrinkage. Should be specify by analyst.

In penalized regression, you need to specify a constant lambda to adjust the amount of the coefficient shrinkage. The best lambda for your data, can be defined as the lambda that minimize the cross-validation prediction error rate. This can be determined automatically using the function cv.glmnet().

Lets dig into the real code

## Computing Ridge Regression

```{r}
# find the best lambda using cross- validation
set.seed(32423)
cv.pen <- cv.glmnet(x.pen, y.pen, alpha = 0)
cv.pen$lambda.min
```

Let's use this lambda
```{r}
model.pen <- glmnet(x.pen, y.pen, alpha = 0, lambda = cv.pen$lambda.min)
# Display regression coefficients
coef(model.pen)
```

Make predictions on the test data
```{r}
x.pen.test <- model.matrix(medv ~., test.data.pen)[,-1]
pred.pen <- model.pen %>% predict(x.pen.test) %>% as.vector()
# model performance metrics
pen.performance <- data.frame(
  RMSE = RMSE(pred.pen, test.data.pen$medv),
  R2 = caret::R2(pred.pen, test.data.pen$medv),
  perError = RMSE(pred.pen, test.data.pen$medv) / mean(test.data.pen$medv)
)
pen.performance 
```
## Computing Lasso Regression

```{r}
# find the best lambda using cross- validation
set.seed(222)
cv.lasso <- cv.glmnet(x.pen,y.pen, alpha =1)
# display the best lambda value
cv.lasso$lambda.min
```

build the model with training data
```{r}
model.lasso <- glmnet(x.pen, y.pen, alpha = 1, lambda = cv.lasso$lambda.min)
# display regression coefficients
coef(model.lasso)
```

Make prediction

```{r}
x.test.lasso <- model.matrix(medv ~., test.data.pen)[, -1]
pred.lasso <- model.lasso %>% predict(x.test.lasso) %>% as.vector()
# model performance metrics
lasso.perf <- data.frame(
  RMSE = RMSE(pred.lasso, test.data.pen$medv),
  R2 = caret::R2(pred.lasso, test.data.pen$medv),
  perError = RMSE(pred.lasso, test.data.pen$medv) / mean(test.data.pen$medv)
)
lasso.perf
```

Last one, elastic net regression

Build the model, it is more simple then the previous twos. 
We can use caret work flow to implement everything. caret will automatically select the best tuning parameters alpha and lambda. Why? because it will 
tests a range of possible alpha and lambda values, and select the best #.

Lets dive in to the code
```{r}
set.seed(32132)
model.elastic <- train(
  medv ~., data = train.data.pen, method = "glmnet",
  trControl = trainControl("cv", number = 10), 
  tuneLength = 10
)
# Best tuning parameter
model.elastic$bestTune
```
Coefficient of the final model. You need to specify the best lambda
```{r}
coef(model.elastic$finalModel, model.elastic$bestTune$lambda)
```

Make Prediction
```{r}
x.test.elastic <-model.matrix(medv ~., test.data.pen)[, -1]
pred.elastic <- model.elastic %>% predict(test.data.pen)
# model performance metrics
elastic.perf <- data.frame(
  RMSE = RMSE(pred.elastic, test.data.pen$medv),
  R2 = caret::R2(pred.elastic, test.data.pen$medv),
  perError = RMSE(pred.elastic,test.data.pen$medv) / mean(test.data.pen$medv)
)
elastic.perf
```

```{r}
penalized.performance <- rbind.data.frame(pen.performance , lasso.perf, elastic.perf)
Pen.Test.Name <- c("Ridge", "Lasso", "Elastic")
penalized.performance <- cbind.data.frame(Pen.Test.Name, penalized.performance)
penalized.performance
```

As you can see the table above, we can either use Lasso model or Elastic.
Imo, i would use Lasso, because of smaller RMSE, slightly higher R2, but more importantly, i have smaller percent error. If i have huge data set, 0.1% does make a different. 

The following code is repeating the process above, but when we use caret 
package, it will be simpler.

Setup a grid range of lambda values
```{r}
lambda <-  10^seq(-3,3, length = 100)
# Ridge Regression
# Build the model
set.seed(123)
model.ridge <- train(
  medv ~., data = train.data.pen, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )
# Model coefficients
coef(model.ridge$finalModel, model.ridge$bestTune$lambda)
# Make predictions
ridge.pred <- model.ridge %>% predict(test.data.pen)
# Model prediction performance
ridge.perf <- data.frame(
  RMSE = RMSE(ridge.pred, test.data.pen$medv),
  R2 = caret::R2(ridge.pred, test.data.pen$medv),
  perError = RMSE(ridge.pred, test.data.pen$medv) / mean(test.data.pen$medv)
)
```

## Lasso Regression
```{r}
# Build the model
set.seed(123)
lasso.model <- train(
  medv ~., data = train.data.pen, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso.model$finalModel, lasso.model$bestTune$lambda)
# Make predictions
lasso.pred <- lasso.model %>% predict(test.data.pen)
# Model prediction performance
lasso.perf <- data.frame(  RMSE = RMSE(lasso.pred, test.data.pen$medv),
                             R2 = caret::R2(lasso.pred, test.data.pen$medv),
                  perError = RMSE(lasso.pred, test.data.pen$medv) / mean(test.data.pen$medv)
)
```

## Elastic net regression
```{r}
# Build the model
set.seed(123)
elastic.model <- train(
  medv ~., data = train.data.pen, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Model coefficients
coef(elastic.model$finalModel, elastic.model$bestTune$lambda)
# Make predictions
predictions <- elastic.model %>% predict(test.data.pen)
# Model prediction performance
elastic.perf <- data.frame(
  RMSE = RMSE(predictions, test.data.pen$medv),
  R2 = caret::R2(predictions, test.data.pen$medv),
  perError = RMSE(predictions, test.data.pen$medv) / mean(test.data.pen$medv)
)

```

The performance of the different models - ridge, lasso and elastic net - can be easily compared using caret. The best model is defined as the one that minimizes the prediction error.
```{r}
modelsss <- list(ridge = model.ridge, lasso = lasso.model, elastic = elastic.model)
resamples(modelsss) %>% summary(metric = "RMSE")
```

Amount all 3 models, elastic has the lowest median RMSE. 

However, if we conclude everything together, lasso would be my first option
```{r}
penalized.performance
```

# Sub- section 4 - rincipal Component and Partial Least Squares Regression Essentials

Intro

This section will be talking about dimension reduction. It is very useful 
if we have a large data set with multiple correlated predictor variables.

The way how reduction methods work is:
First summarizing the original predictors into few new variables called 
principal components (PCs), which are used as predictors to fit the linear regression model. It will avoid multicollinearity between predictors, which
is a big issue in regression setting.

When using dimension reduction methods, it's generally recommended to 
standardize each predictor to make them comparable. Standardization consists of dividing the predictor by its standard deviation. 

Two well known regression methods based on dimension reduction:
Principal Component Regression (PCR)
Partial Least Squares (PLS)

Principal Component Regression

PCR means summarize the original predictor variables into few new variables 
also known as principal components (PC), which are a linear combination of 
the original data. 

Those PC are used to build the linear regression model. Those PC are chosen by cross-validation (CV). 

Note that, PCR is suitable when the data set contains highly correlated predictors.

Partial Least Squares Regression

Downside of PCR is that, we cant guarantee that the selected principal
components are associated with the outcome. The selection of the principal components that are included in the model is not supervised by the outcome variable. 

The alternative to PCR is Partial Least Squares (PLS) Regression.
PLS identifies new principal components that not only summarizes the original predictors, but also that are related to the outcome. Then those components are used to fit the regression model. Compared to PCR, PLS uses a dimension 
reduction strategy that is supervised by the outcome. 

Like PCR, PLS is convenient for data with highly- correlated predictors.
The # of PCs used in PLS is generally chosen by the Cross-Validation. To make variables comparable, predictors and the outcome variables should be generally standardized. 
Prepare the data 

```{r cars}
library(tidyverse)
library(caret)
library(pls)
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
```

## Computing Principal Component Regression

```{r}
# Build the model on training set
set.seed(123)
model <- train(
  medv~., data = train.data, method = "pcr",
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Plot model RMSE vs different values of components
plot(model)
```

From the plot, we can see that 6 or 10 is our best principal components number. which also gives the smallest prediction error RMSE. 

Without guessing we can use the following code

```{r}
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model$bestTune
```
It said, the best model is 10

```{r}
# Summarize the final model
summary(model$finalModel)
```
From the last two rows, we want big number. For 10 comps, 96.92% of 
the variation (or information) contained in the predictors are captured by 6 principal components. Also, setting ncomp = 10 will captures 70.64% of the information in the outcome variable, which is pretty good.

```{r}
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance metrics
pcr.performance <-data.frame(
  RMSE = caret::RMSE(predictions, test.data$medv),
  Rsquare = caret::R2(predictions, test.data$medv),
  perError = caret::RMSE(predictions, test.data$medv) / mean(test.data$medv)
)
```

2 
```{r}
# Build the model on training set
set.seed(123)
model <- train(
  medv~., data = train.data, method = "pls",
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Plot model RMSE vs different values of components
plot(model)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model$bestTune
# Summarize the final model
summary(model$finalModel)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance metrics
pls.performance <- data.frame(
  RMSE = caret::RMSE(predictions, test.data$medv),
  Rsquare = caret::R2(predictions, test.data$medv),
  perError = caret::RMSE(predictions, test.data$medv) / mean(test.data$medv)
)
rbind.data.frame(PCR = pcr.performance, PLS = pls.performance)
```

Unfortunately, we get exactly the same data, so we can either use both model.

Comment: 

I have been suffering from the following error
"Error in UseMethod("R2") : 
  no applicable method for 'R2' applied to an object of class "c('double', 'numeric')""

The way I fixed this problem is to add caret::R2()


Section 4 - Classification methods essential -  total sub sections 9 

Classification Methods Essentials 

From the previous 3 sections, we have talked about how to predict a quantitative or continuous outcome variables based on one or multiple
predictor variables. 
 
In this section, classification, the out come variables are qualitative
(or categorical). Classification means a set of machine learning methods for prediction the class (or category) of individuals on the basis of one of multiple predictor variables. 

Topics we be talked about in this section: 

- Logistic regression, for binary classification tasks

- Stepwise and penalized logistic regression for variable selections

- Logistic regression assumptions and diagnostics

- Multi-nomial logistic regression, an extension of the logistic regression for multi-class classification tasks

- Discriminant analysis, for binary and multi-class classification problems 

- Naive Bayes classifier

- Support vector machines 

- Classification model evaluation 

Classification simply refer to what type of thing is it. For example, input is apple, when you pass it in to those methods above, it will try to find out what is your input, calculated in probability, and tell you what is the input. 

You would need a cutoff point for probability so that you can make things work. 

Sub-section 1 - Logistic Regression Essential in R

Let get started with Logistic Regression Essentials in R

In Logistic Regression, it will tell you yes or no. Is it an apple? Yes / No 
thats it. 

The equation usually comes in individuals based on one or multiple predictor variables (x). It returns Y / N like Bernoulli or Binomial distribution, success of failure.

Logistic Regression comes from a family called Generalized Linear Model (GLM). It is a extension of linear regression model. Other synonyms are binary logistic regression, binomial logistic regression, and logit model. 

Logistic Regression does not return directly the class of observation, it tells you the probability if it belongs to that specific class or not. So thats mean you need to decide the threshold probability at which the category flips from one to the other. By default, it is set to p = 0.5. However, in reality it should be settled based on the analysis purpose. 

This section you will learn how to:

- Define the logistic regression equation and key terms such as
log-odds and logit

- Perform Logistic Regression in R and interpret the results.

- Make predictions on new test data and evaluate the model accuracy

You will need to know what is logistic regression.

Here is the formula
- Log[p/(1-p)] = b0 + b1 * x + b2 * X2 ... + bn * Xn

Probability can be calculated from the odds as  p = Odds/(1 + Odds)

```{r}
library(tidyverse)
library(caret)
theme_set(theme_bw())
```

Preparing the data

Logistic Regression works for continuous and / or categorical predictor variables. 

Performing the following steps might improve the accuracy of your model
- remove potential outliers

- make sure that the predictor variables are normally distributed. if not, you
  use log, root, box-cox transformation. 

- Remove highly correlated predictors to minimize over fitting. The presence
  of highly correlated predictors might lead to an unstable model solution. 

We will be using PimaIndiansDiabetes2 from mlbench package. 

Let get into the code. ( dont for get to install the packages if you havent)

```{r}
set.seed(3213)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# split the data 
training.samples.in <- PimaIndiansDiabetes2$diabetes %>%
  createDataPartition(p = 0.8, list = F)
train.data.in <- PimaIndiansDiabetes2[training.samples.in, ]
test.data.in <- PimaIndiansDiabetes2[-training.samples.in, ]
```

Computing Logistic Regression
glm() -> generalized linear model, can be used to compute  logistic regression.
you will need to specify the option family = binomial, which tells R that we want to fit logistic regression.

```{r}
# fit the model
model.in <- glm(diabetes ~., data = train.data.in, family = binomial)
# summarize the model 
summary(model)
# make predictions
probabilities <- model.in %>% predict(test.data.in, type = "response")
pred.in <- ifelse(probabilities > 0.5, "pos", "neg")
# model accuracy
mean(pred.in == test.data$diabetes)
```

## Simple Logistic Regression

when you see simple, most likely it refers to one predictor variable.

The following R code builds a model to predict the probability of being diabetes positive based on the plasma glucose concentration.

```{r}
model.slr <- glm(diabetes ~ glucose, data = train.data.in, family = binomial)
summary(model.slr)$coef
```

Both beta0 and beta1 are at significance level.

Now the logistic equation can be written as
p = exp(-6.32 + 0.043 * glucose) / [ 1 + exp(-6.32 + 0.043 * glucose)]

With this formula, each new glucose plasma concentration value, you can predict the probability of the individuals in being diabetes positive.

We can use predict() and use type = "response" to directly obtain the probabilities

```{r}
newdata.in <- data.frame(glucose = c(20,180))
prob.in <- model.slr %>% predict(newdata.in, type = "response")
pred.class <- ifelse(probabilities > 0.5, "pos", "neg")
pred.class
```

We can plot the model

```{r}
train.data.in %>%
  mutate(prob = ifelse(diabetes == "pos", 1, 0)) %>%
  ggplot(aes(glucose,prob)) + 
  geom_point(alpha = 0.2) +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  labs(
    title = "Logistic Regression model",
    x = "Plasma Glucose Concentration",
    y = "Probability of being diabete - pos")
```

## Multiple logistic regression

```{r}
model.mlr <- glm(diabetes ~., data = train.data.in, family = binomial)
summary(model.mlr)$coef
summary(model.mlr)
```

From the table above, all those betas are at their significance levels, are they?

remember we use 0.05. so for those has less than 02, they are actually not significance. why? is 0.3 smaller than 0.05? no so 0.3 is not signifigant at all... Dont mess up this part.

Estimate is the coefficient for that specific beta. 

Std.Error is the standard error of the coefficient estimates. This is also representing the accuracy of the coefficients. We want small numbers from this col.

z value is the z statistic, which is the coefficient estimate (col 2) divided by the standard error of the estimate (col3) 

Pr(>|z|) is the p value corresponding to the z statistic. We want small value from this. 

So let clean up the formula
```{r}
model.mlr2 <- glm(diabetes ~ pregnant + glucose + mass,
                  data = train.data.in, 
                  family = binomial)
summary(model.mlr2)
```

## Interpretation

Coding wise, everything above is the most important thing you will need to know. Also, there is another concept. 

for example, in our data, we know the the coefficient for glucose is 0.036900.
It means if we have 1 unit of glucose glucose -> e^0.036900 = 
```{r}
exp(0.036900)
```
So it will increase 1.037589. 
so if we have large coefficient for beta, it is very bad. just like pregnant. 
```{r}
exp(0.176226)
```

We will make predictions using the test data in order to evaluate the performance of our logistic regression model.

```{r}
prob.test <- model.mlr2 %>% predict(test.data.in, type = "response")
head(prob.test)
```

let convert those number to either pos or neg
```{r}
pred.class.test <- ifelse(probabilities > 0.5, "pos", "neg")
head(pred.class.test)
```

Assessing model accuracy
```{r}
mean(pred.class.test == test.data.in$diabetes)
```
From here, we can see that we have 25.64103% error rate. So our model is alright, not too bad. 

In this section, we have talked about how logistic regression works. We have gone thru how to make predictions and assess the model accuracy. Logistic model output is very easy to interpret compared to other classification methods. The reason why it is easy to interpret compared to other classification methods. Also, because of it's simplicity, we will less likely to have over fitting problem than flexible methods, like decision trees.

A lot of concepts from linear regression will remain it's ground for logistic regression modeling. Look at what we did towards the end of the section. We still perform some diagnostics to make sure that the assumptions made by the model are met for our data. In addition, we need to see how good the model is in predicting the outcome for the new test data set. We also have gone thru the classification accuracy, but not that other important performance metric exist. 

If we have a lot of parameters, we might want to use stepwise regression, lasso regression techniques to see which parameters are significant.  We can also add interaction terms in the model or include the spline terms. We can also fit generalized additive models when linearity of the predictor cannot be assumed. this can be done using mgcv package. 

```{r}
library("mgcv")
# fit the model
gam.model <- gam(diabetes ~ s(glucose) + mass + pregnant, data = train.data.in, family = "binomial")
# summarize model
summary(gam.model)
prob.gam <- gam.model %>% predict(test.data.in, type = "response")
pred.gam <- ifelse(probabilities > 0.5, "pos", "neg")
# model accuracy
mean(pred.gam == test.data.in$diabetes)

```

As we know, Logistic regression is limited to only two class classification. If we want more outcome, we might need multi-nomial logistic regression for multi-class classification problem. 

# Sub-section 2 - Stepwise Logistic Regression Essentials in R

In this section, we will be talking about stepwise logistic regression that consists of automatically selecting a reduced number of predictor variables for building the best performing logistic regression model. 

load up and  prepare the data 
```{r}
library(tidyverse)
library(caret)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```
Compute the stepwise logistic regression

The stepwise logistic regression can be easily computed using the R function stepAIC() available in the MASS package. It performs model selection by AIC. It has an option called direction, which can have the following values: “both”, “forward”, “backward”

Full model
```{r}
full.model <- glm(diabetes ~., data = train.data, family = binomial)
coef(full.model)
```

## Perform Stepwise variable selection
```{r}
step.model <- full.model %>% stepAIC(trace = FALSE)
coef(step.model)
```

Compare the full and the stepwise model
```{r}
probabilities <- full.model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
```

```{r}
probabilities <- predict(step.model,test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
oberseved.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
```

As you can see, we have a better model after applying the stepwise function 
and selection the most impactful parameters to the model.

Just to remember, the more simpler model the better it is. 

Sub-section 3 - Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net

Penalized logistic regression imposes a penalty to the logistic model for having to many variables (parameters). This will reduce / lower the coefficients those parameters who have less contribution to the model towards to zero. This entire process is also called regularization. 

Most common penalized regression include:

Ridge regression: Will set less meaningful parameters close to zero

Lasso regression: will set less meaningful parameters to exactly zero

Elastic net regression: combination of ridge and lasso regression.

In this section, we will penalized logistic regression, by lasso regression for automatically selecting the optimal and most contributing parameters to the model. 

Let get in to the code
```{r}
library(tidyverse)
library(caret)
library(glmnet)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
set.seed(1523)
training.samples <- PimaIndiansDiabetes2$diabetes %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
# Dumy code categorical predictor variables
x <- model.matrix(diabetes~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$diabetes == "pos", 1, 0)
```

## Lasso 

```{r}
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
model <-glmnet(x,y, alpha = 1, family = "binomial",
               lambda = cv.lasso$lambda.min)
coef(model)
x.test <- model.matrix(diabetes ~., test.data)[,-1]
prob <- model %>% predict(newx = x.test)
pred <- ifelse(probabilities > 0.5, "pos", "neg")
ob.classes <- test.data$diabetes
mean(pred == ob.classes)
```

```{r}
plot(cv.lasso)
```

The plot displays the cross-validation error according to the log of lambda. the left dashed vertical line indicates that the log of the optimal value of lambda is approximately -4 which is the one that minimizes the prediction error, where will give you the most accurate model. 

```{r}
cv.lasso$lambda.min
```

Generally speaking, the purpose of regularization is to balance accuracy and simplicity, which implies that the smallest number of predictors that also gives the highest accuracy model. the function cv.glment() finds also the value of lambda that gives the simplest model, but also lies within one standard error of the optimal value of lambda. This value is called lambda.1se
```{r}
cv.lasso$lambda.1se
```
use lambda.min as the best lambda
```{r}
coef(cv.lasso, cv.lasso$lambda.min)
```

using lambda.1se as the best lambda
```{r}
coef(cv.lasso, cv.lasso$lambda.1se)
```
 
The next code, we will compute the final model using lambda.min and then access 
the model accuracy against the test data.

```{r}
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(diabetes ~., test.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)

```

lambda.1se
```{r}
# Final model with lambda.1se
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)
# Make prediction on test data
x.test <- model.matrix(diabetes ~., test.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy rate
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
```

## Compute the full logistic model
```{r}
# Fit the model
full.model <- glm(diabetes ~., data = train.data, family = binomial)
# Make predictions
probabilities <- full.model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)

```

According to the author, even tho we have a 0.7948718 accuracy for the full
logistic model, we would pick lasso.min as our lambda. Because the model is 
far less complicated. According to the bias variance trade off, all things
equal, simpler model should be always preferred because it is less likely 
to overfit the training data.

For variable selection, an alternative to the penalized logistic regression is the stepwise logistic regression.

# Sub-section 4 - Logistic Regression Assumptions and Diagnostics in R

This section will go thru the major assumptions and provides practical 
guide to check whether these assumptions hold true for your data, which
means it this section will tell us if we are building a good model. 

Key concept:

Outcome for Logistic regression is either yes or no 1 or 0 or positive or negative

There is a linear relationship between the logit of the outcome and each predictor variables. logit(p) = log(p/(1-p)), where p is the probabilites of the outcome. 

There is no influential values ( extremevalues or outliers) 

there is no high intercorrelations (multi collinearity) among the predictors 

To have a better accuracy of your model, we need to make sure those assumption = T

```{r}
library(tidyverse)
library(broom)
theme_set(theme_classic())
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Fit the logistic regression model
model <- glm(diabetes ~., data = PimaIndiansDiabetes2, 
               family = binomial)
# Predict the probability (p) of diabete positivity
probabilities <- predict(model, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)
```

## Logistic regression diagnostics

Linearity assumption

Simply means we plot the data and check if they are linear or not.

We need to remove qualitative variables from the original data frame and bind the logit values to the data

```{r}
# select only numeric predictors
mydata <- PimaIndiansDiabetes2 %>% dplyr::select_if(is.numeric)
predictors <- colnames(mydata)
# bind the logit and tiding the data for plot
mydata <- mydata %>% mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
```

Create a scatter plots
```{r}
ggplot(mydata, aes(logit, predictor.value)) + 
  geom_point(size = 0.5, alpha =0.5) + 
  geom_smooth(method = "loess") +
  facet_wrap(~predictors, scales = "free_y") +
  theme_bw()
```

As we can see, glucose, mass, pregnant, pressure and triceps are all quite linearly associated with the diabetes outcome in logit scale. Which means age and pedigree is not linear so that they might need transformations. What should we do? poly, fractional poly, spline would work

## Influential values 

We can view those extreme values in the data by visualizing the Cook's distance values with n = 3
```{r}
plot(model, which = 4, id.n=3)
```
Not all outliers are influential observations. We can check standardized residual error for potential influential observations. 

```{r}
# extract model results
model.data <- augment(model) %>% mutate(index = 1:n())
model.data %>% top_n(3, .cooksd)
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = diabetes), alpha = .5) + 
  theme_bw()
```

Filter potential influential data points with abs(.std.res) > 3

```{r}
model.data %>% filter(abs(.std.resid) >3)
```
empty which means we do not have any influential observations in our data. 

If i have outliers in a continuous predictor, potential solutions include:

1) Removing the concerned records
2) Transform the data into log scale
3) use non parametric methods

Multi-collinearity

Multi-collinearity means one or more parameters are super highly correlated, which is an important task in regression analysis and should be fixed by removing those specific variables. we can use vif() to cehck 

we dont want it to exceeds 5 or 10. Otherwise those are the problem.

```{r}
car::vif(model)
```

## Discussion

In this section, we have describes the main assumptions of logistic regression
model, and provides examples of R code to diagnose potential problems in the
data including non linearity between the predictor variables and the logit
of the outcome. The present of influential observations in the data and 
multicollinearity among predictors.Fixing those potential problems might 
improve considerably the goodness of the model.

# Sub-section 5  - Multi nomial Logistic Regression Essentials in R

Multinomial logistic regression is an extension of the logistic regression
for multi class classification tasks. It is used when the outcome involves
more than two classes.

nnet ->  computing multinomial logistic regression

```{r}
library(tidyverse)
library(caret)
library(nnet)
# load up the data
data("iris")
# split the data
set.seed(3333)
training.samples <- iris$Species %>% createDataPartition(p = 0.8, list = F)
train.data <- iris[training.samples, ]
test.data <- iris[-training.samples, ]
```

## Computing multinomial logistic regression

```{r}
# feed the model
model <- nnet::multinom(Species ~., data = train.data)
summary(model)
pred.classes <- model %>% predict(test.data)
head(pred.classes)
```

```{r}
# model accuracy
mean(pred.classes == test.data$Species)
```

Thats a pretty high accuracy.

Discussion 

So we have gone thru multinomial logistic regression in R. This method is used for multi classes problems. However, we dont use it that often. However, Discriminant analysis is more popular for multiple class classification.


# Sub-section 6 - Discriminant Analysis Essentials in R

Discriminant analysis is used to predict the probability of belonging to a given class based on one or multiple predictor variables. It works with continuous / 
categorical predictor variables. 

In discriminant analysis is more suitable for predicting the category of an
observation in the situation where the outcome variable contains more than
two classes. in addition, it's more stable than the logistic regression for multi-class classification problems. 

Remember, both logistic and discriminant analysis can be used for binary classification. 

Here are the discriminant analysis methods will be discussed

- Linear discriminant analysis (LDA) it uses linear combinations of predictors
  to predict the class of a given observation. Assumes that the predictor variables   (p) are normally distributed and the classes have identical variances 
  (for univariate analysis, p = 1) or identical covariance matrices 
  (for multivariate analysis, p >1)

- Quadratic discriminant analysis (QDA) more flexible than LDA. There is no 
  assumption that the covariance matrix of classes is the same 

- Mixture discriminant analysis (MDA) each class is assumed to be a Gaussian
  mixture of sub classes

- Flexible Discriminant Analysis (FDA) non linear combinations of predictors 
  are used such as splines 

- Regularized Discriminant Analysis (RDA) Regularization (shrinkage) improves
  the estimate of the covariance matrices in situations where the number of 
  predictors are larger than the number of samples in the training data. This
  leads to an improvement of the discriminant analysis. 


```{r}
library(tidyverse)
library(caret)
theme_set(theme_classic())
data("iris")
set.seed(321312)
training.samples <- iris$Species %>% createDataPartition(p =0.8, list =F)
train.data <- iris[training.samples, ]
test.data <- iris[-training.samples, ]
# Normalize the data. Categorical variables are automatically ignored. 
# Estimate pre processing parameters 
preproc.param <- train.data %>% preProcess(method = c("center", "scale"))
# transform the data using the estimated parameters
train.transf <- preproc.param %>% predict(train.data)
test.transf <- preproc.param %>% predict(test.data)
```

## Linear Discriminant Analysis LDA

LDA starts by finding directions that maximize the separation between classes. 
Then use those directions to predict the class of individuals. These directions, called linear discriminant, are a linear combinations of predictor variables.

LDA assumes that predictors are normally distributed and that the different classes have class- specific means and equal variance / covariance. 

before performing LDA, consider: 

- Inspecting the univariate distributions of each variable and make sure that they are normally distributed. If not, you can transform them using log and root for exponential distributions and box-cox for skewed distributions. 

- Removing outliers from your data and standardize the variables to make their scale comparable. 

The linear discriminant analysis can be easily computed using the function lda()
from the mass package 

```{r}
library(MASS)
# feed the model
model <- lda(Species ~., data = train.transf)
pred <- model %>% predict(test.transf) 
# model accuracy
mean(pred$class ==test.transf$Species)
model
```

LDA determines group means and computes for each individual, the probability of 
belonging to the different groups. The individual is then affected to the group
with the highest probability score. 

lda() outputs contain the following elements:

- Prior probabilities of groups: the proportion of training observations in each group. for example, there are 31% of the training observations in the samosa group

- Group means: group center of gravity. Shows the mean of each variable in each group 

- coefficients of linear discriminants: shows the linear combination of predictor
variables that are used to form the LDA decision rule. For example:
LD1 = 0.8055912 * Sepal.Length + (Sepal.Width * 0.7225173)
      - (Petal.Length * 3.6651301) - (Petal.Width * 2.4644317)
LD2 = (Sepal.Length * 0.1269374) + (Sepal.Width * 0.9781999)
      - (Petal.Length * 1.6646138) +  (Sepal.Width * 2.1330713) 


```{r}
model <- lda(Species~., data = train.transf)
plot(model)
```

```{r}
pred <- model %>% predict(test.transf)
names(pred)
```

The predict() function returns the following elements.

Class: predicted classes of observations
Posterior: is a matrix whose columns are the groups, rows are the individuals
and values are the posterior probability that the corresponding observation
belongs to the groups
x: contains the linear discriminant, described above

```{r}
# predicted classes
head(pred$class,6)
# predicted probabilities of class membership
head(pred$posterior, 6)
# linear discriminants
head(pred$x, 3)
```

create the LDA plot using ggplot2
```{r}
lda.data <- cbind(train.transf, predict(model)$x)
ggplot(lda.data, aes(LD1,LD2)) + 
  geom_point(aes(color = Species))
```

Model accuracy
```{r}
mean(pred$class == test.transf$Species)
```

we have a super high model accuracy

Note that, by default, the probability cutoff used to decide group-membership
is 0.5 (random guessing). For example the number of observations in the setosa group can be recalculated using:
```{r}
sum(pred$posterior[,1] >= .5)
```

In some situations, you might want to increase the precision of the model.
In this case you can fine-tune the model by adjusting the posterior probability
cutoff. For example, you can increase or lower the cutoff.

Variable selection:
note that, if the predictor variables are standardized before computing LDA, 
the discriminator weights can be used as measures of variable importance for
feature selection. 

## Quadratic Discriminant analysis - QDA

QDA is little bit more flexible than LDA, in the sense that it does not assumes the equality of variance / covariance. in other words, for QDA the co variance matrix can be different for each class. 

LDA tends to be a better than QDA when you have a small training set. 

QDA is recommended if the training set is very large, so that the variance 
of the classifier is not a major issue, or if the assumption of a common
covariance matrix for the k classes is clearly untenable

QDA can be computed using the R function qda() from the mass package

```{r}
library(MASS)
# fit the model 
model <- qda(Species ~., data = train.transf)
model
```

```{r}
# make predictions 
pred <- model %>% predict(test.transf)
# model acccracy
mean(pred$class == test.transf$Species)
```

## Mixture Discriminant analysis - MDA

The LDA classifier assume that each class comes from a single normal (or Gaussian) distribution. this is too restrictive. For MDA, there are classes,
and each class is assumed to be a Gaussian mixture of sub classes, where each data point has a probability of belonging to each class. Equality of co variance matrix, among classes, is still assumed. 

```{r}
library(mda)
model <- mda(Species ~., data= train.transf)
model
```

make predictions
```{r}
pred.classes <- model %>% predict(test.transf)
# model accuracy
mean(pred.classes == test.transf$Species)
```

## Fliexible discriminant analysis - FDA

FDA is a more flexible extension of LDA that uses non linear combinations of predictors such as splines. FDA is useful to model muli variate non-normality
or non linear relationships among variables within each group for allowing a more accurate classification. 

```{r}
library(mda)
model <- fda(Species ~., data = train.transf)
predict.classes <- model %>% predict(test.transf)
mean(predict.classes == test.transf$Species)
```

## Regularized discriminant analysis

RDA builds a classification rule by regularizing the group covariance matrices. Allowing a more robust model against multicollinearity in the data. It could be useful for large multivariate data set containing highly correlated predictors.

Regularized discriminant analysis is a kind of a trade - off between LDA and QDA. Recall that, in LDA we assume equality of covariance matrix for all of the classes. QDA assuems different covariance matrices for all the classes. Regularized discriminant analysis is an intermediate between LDA and QDA.

RDA shrinks the separate covariances of QDA toward a common co variance as in LDA. this improves the estimate of the covariance matrices in situations where the number of predictors is larger than the number of samples in the training data, potentially leading to an improvement of the model accuracy. 


```{r}
library(klaR)
model <- rda(Species ~., data = train.transf)
pred <- model %>% predict(test.transf)
mean(pred$class == test.transf$Species)
```

## Discussion 

We have descirbed linear discriminant analysis (LDA) and extensions for predicting the class of an observations based on multiple predictor variables. Discriminant analysis is more suitable to multi class classification problems compared to the logistic regression. 

LDA assumes that the different classes have the same variacne or covariacne matrix. we have described many extension of LDA in this section. the most popular extension of LDA is the quadratic discriminant analysis (QDA) which is more flexible than LDA in the sense that it does not assume the quality of the group covariance matrices.

LDA tends to be better than QDA for small data set. QDA is recommended for large training data set. 

# Sub-section 7 - Naive Bayes Classifier Essentials

The Naive Bayes classifier is a simple and powerful method that can be used for binary and multiclass classification problems. 

Naive Bayes classifier predicts the class membership probability of observations using Bayes theorem, which is based on conditional probability

Observations are assigned to the class with the largest probability score. 

We will be using klaR and caret package in this sections

```{r}
library(tidyverse)
library(caret)
library(klaR)
data("PimaIndianDiabetes2", package ="mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
set.seed(3323)
training.samples <- PimaIndiansDiabetes2$diabetes %>% 
  createDataPartition( p =0.8, list=F)
train.data <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```

## Computing Naive Bayes
```{r, warning = FALSE}
library("klaR")
model <- NaiveBayes(diabetes ~., data = train.data)
pred <- model %>% predict(test.data)
mean(pred$class == test.data$diabetes)
```

## Using caret R package
```{r, warning = FALSE}
library(klaR)
set.seed(3233)
model <- train(diabetes ~., data = train.data, method ="nb",
               trControl = trainControl("cv", number =10))
predicted.classes <- model %>% predict(test.data)
mean(predicted.classes == test.data$diabetes)
```

## Discussion 

This chapter introduces the basics of Naive Bayes classification and provides practical examples in R using the klaR and caret package.

# Sub-section 8 - SVM Model: Support Vector Machine Essentials

Support Vector Machine (SVM) is a machine learning technique used for classification tasks. SVM will create a boundaries. let n = group, boundaries = n-1. 3 groups i will have 2 lines that separates the data points. The line that im referring to can be a linear / non linear. it can be used for two class or multi - class classification problems. 

In practice, those are usually non linear. Technically, svm algorithm perform a non linear classification using kernel trick. Most common kernel transformations are polynomial / radial kernel. 

Also, there is also an extension of the SVM for regression which is called support vector regression. 

In this chapter, we will be building SVM classifier using the caret R package

```{r}
library(tidyverse)
library(caret)
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
set.seed(123)
training.samples <- pima.data$diabetes %>% 
  createDataPartition( p = 0.8, list = F)
train.data <- pima.data[training.samples, ]
test.data <- pima.data[-training.samples, ]
```

## SVM linear Classifier
```{r}
set.seed(123)
model <- train(diabetes ~., data = train.data, method = "svmLinear", 
              trControl = trainControl("cv", number = 10),
              preProcess = c("center", "scale"))
pred.classes <- model %>% predict(test.data)
head(pred.classes)
```

## Compute model accuracy rate
```{r}
mean(pred.classes == test.data$diabetes)
```

There is a parameter C, also known as cost. the C determines the possible of misclassifications. It end up imposes a penalty to the model for making an
error. Big C = less likely SVM algorithm will mis classify a point.
So we want Big C

by default caret builds the SVM linear classifier using C = 1. you can check this by typing model in R console. It is possibly to check all C and choose the optimal one to maximize the model cross validation accuracy. 

```{r}
set.seed(123)
model <- train(
  diabetes ~., data = train.data, method = "svmLinear", 
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(C = seq(0,2,length = 20)),
  preProcess = c("center", "scale"))
plot(model)
```


Get the best C
```{r}
model$bestTune
# pred with best tunned data
predicted.classes <- model %>% predict(test.data)
mean(predicted.classes == test.data$diabetes)
```

## SVM classifier using Non-Linear Kernel

To build a non linear SVM classifier, we need to use either polynomial kernel 
or radial kernel function. the caret package can be used to easily computes
the polynomial and the radial SVM non linear models.
The package automatically take the optimal values for the model tuning parameters, 
where optimal is defined as values the maximize the model accuracy.

Computing SVM using radial basis kernel 

```{r}
set.seed(123)
model <- train(diabetes ~., data = train.data, method = "svmRadial",
               trControl = trainControl("cv", number = 10), 
               preProcess = c("center", "scale"),
               tuneLength = 10)
model$bestTune
```

make prediction
```{r}
pred.classes <- model %>% predict(test.data)
mean(pred.classes == test.data$diabetes)

```

## Compute SVM using polynomial basis kernel
```{r}
set.seed(123)
model <- train(diabetes~., data = train.data, method = "svmPoly",
               trControl = trainControl("cv", number = 10),
               preProcess = c("center", "scale"),
               tuneLength = 4)
model$bestTune
# make prediction
pred.classes <- model %>% predict(test.data)
mean(pred.classes == test.data$diabetes)
```

## Discussion

looks like our SVM linear model has the highest accuracy. therefore we will 
use that as our final model. 


## Sub - section 9 - Evaluation of Classification Model Accuracy: Essentials

After building a classification model, i will need to evaluate the performance
of the model.

So i will need to estimate the model prediction accuracy and prediction errors using a new test data set. because we know the actual outcome of observations in the test data, the performance of the predictive model can be accessed by comparing the predicted outcome values against the known outcome values.

We will be covering the commonly used metrics and methods for assessing the performance of predictive classification model:

Average classification accuracy, representing the proportion of correctly classified observations.

Confusion matrix, which is 2x2 table showing four parameters, including the number of 
true positives, true negatives, false negatives and false positive

Precision, Recall and Specificity which are three major performance metrics describing a ppredictive classification model

ROC curve, which is a graphical summary of the overall performance of the model, showing the proportion of true positives and false positives at all possible values of probability cutoff. the area under the curve (AUC) summarizes the overall performance of the classifier. 

build a classification model
```{r}
library(tidyverse)
library(caret)
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
set.seed(123)
training.samples <- pima.data$diabetes %>% 
  createDataPartition(p = 0.8, list =F)
train.data <- pima.data[training.samples, ]
test.data <- pima.data[-training.samples,]
```


use lda model
```{r}
library(MASS)
fit <- lda(diabetes~., data = train.data)
pred <- predict(fit, test.data)
pred.prob <- pred$posterior[, 2]
pred.classes <- pred$class
observed.classes <- test.data$diabetes
```

## Overall Classification accuracy
The overall classification accuracy rate corresponds to the proportion of observations that have been correctly classified. Determining the raw classification accuracy is the first step in assessing the performance of a model.

Inversely, the classification error rate is defined as the proportion of observations that have been misclassified. Error rate = 1 - accuracy

The raw classification accuracy and error can be easily computed by comparing the observed classes in the test data against the predicted classes by the model:

```{r}
accuracy <- mean(observed.classes == pred.classes)
accuracy
```

```{r}
error <- mean(observed.classes != pred.classes)
error
```

From the output, we can see the the lienar discriminant analysis correctly predicted the individual outcome in 77% of the cases. This is by far better than random guessing. The misclassification error rate can be calculated as
100 - 77 = 23

In our example, a binary classifier can make two types of erros 

diabetes positive to diabetes negative
diabetes negative to diabetes positive

we can use confusion matrix to see the predicted outcome values against the known outcome values and see the error 

## Confusion matrix

We can use table() to produce a confusion matrix in order to find the number of observations were correctly or incorrectly classified. It compares the observed and the predicted outcome values and shows the number of correct and incorrect predictions categorized by type of outcome 

```{r}
# Confusion matrix, number of cases 
table(observed.classes, pred.classes)
```

```{r}
table(observed.classes, pred.classes) %>% prop.table() %>% round(digits = 4)
```

The diagonal elements of the confusion matrix indicate correct predictions, while the off diagonals represent incorrect predictions. 
Correct prediction -> 0.5651 and 0.2051 
So the correct classification rate is the sum of the number of case on the diagonal divided by the sample size in the test data. 
```{r}
(44+16)/(nrow(test.data))
```

Please look at the table from the below links 
<https://bit.ly/2JDOw69>

each cell has it's own meaning

Cell a: we predicted diabetes - negative and the individuals were diabetes - negative

Cell b: We predictd diabetes - positive, but the individuals didn't actually have diabetes.  (type 1 error)

Cell c: We predicted diabetes - negative, but they did have diabetes
(Type 2 error)

Cell d: These are cases in which we predicted the individuals would be diabetes positive and they were. 

So diagonal is the correct prediction. So the non diagonal numbers are the wrong prediction. 

## Precision, Recall and Specificity 

In addition to the raw classification accuracy, there are many other metrics that are widely used to examine the performance of a classification model, including:

Precision, which is the proportion of true positives among all the individuals that have been predicted to be diabetes-positive by the model. This represents the accuracy of a predicted positive outcome. Precision = TruePositives/(TruePositives + FalsePositives).

Sensitivity (or Recall), which is the True Positive Rate (TPR) or the proportion of identified positives among the diabetes-positive population (class = 1). Sensitivity = TruePositives/(TruePositives + FalseNegatives).

Specificity, which measures the True Negative Rate (TNR), that is the proportion of identified negatives among the diabetes-negative population (class = 0). Specificity = TrueNegatives/(TrueNegatives + FalseNegatives).

False Positive Rate (FPR), which represents the proportion of identified positives among the healthy individuals (i.e. diabetes-negative). This can be seen as a false alarm. The FPR can be also calculated as 1-specificity. When positives are rare, the FPR can be high, leading to the situation where a predicted positive is most likely a negative.

Sensitivy and Specificity are commonly used to measure the performance of a predictive model.

These above mentioned metrics can be easily computed using the function confusionMatrix() [caret package].

In two-class setting, you might need to specify the optional argument positive, which is a character string for the factor level that corresponds to a “positive” result (if that makes sense for your data). If there are only two factor levels, the default is to use the first level as the “positive” result.

```{r}
confusionMatrix(pred.classes, observed.classes, positive = "pos")
```

The model accuracy is around 76.92%. 

In our example, sensitivity is 61.54%, which implies that diabetes-positive individuals that were correctly identified by the model as diabetes-positive

The specificity of the model is 84.62% which impies that the proportion of diabetes-negative individuals that were correctly identified by the model as diabetes-negative. 

The model precision or the proportion of positive predicted value is 66.67%

In the medical science, sensitivity and specificity are two important metrics that characterize the performance of classifier or screening test. In medical diagnostic, we are concerned about the minimal wrong positive diagnosis. So we want high specificity, which we have 84.62%. To improve the sensitivity / precision, we need to test different probability cutoff for test positive / negative. 

## ROC curve
ROC curve (receiver operating characteristics curve) is a popular graphical measure for the performance or the accuracy of a classifier, which corresponds to the total proportion of correctly classified observations. 

For example, the accuracy of a medical diagnostic test can be assessed by considering the two possible types of errors: false positives, and false negatives. In classification point of view, the test will be declared positive when the corresponding predicted probability, returned by the classifier algorithm, is above a fixed threshold. This threshold is generally set to 0.5 (i.e., 50%), which corresponds to the random guessing probability.

So, in reference to our diabetes data example, for a given fixed probability cutoff:

The true positive rate (or fraction) is the proportion of identified positives     among the diabetes-positive population. Recall that, this is also known as     the sensitivity of the predictive classifier model.
And the false positive rate is the proportion of identified positives among the     healthy (i.e. diabetes-negative) individuals. This is also defined as           1-specificity, where specificity measures the true negative rate, that is     the proportion of identified negatives among the diabetes-negative             population.

Since we don’t usually know the probability cutoff in advance, the ROC curve is typically used to plot the true positive rate (or sensitivity on y-axis) against the false positive rate (or “1-specificity” on x-axis) at all possible probability cutoffs. This shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something. Another visual representation of the ROC plot is to simply display the sensitive against the specificity.

The Area Under the Curve (AUC) summarizes the overall performance of the classifier, over all possible probability cutoffs. It represents the ability of a classification algorithm to distinguish 1s from 0s (i.e, events from non-events or positives from negatives).

For a good model, the ROC curve should rise steeply, indicating that the true positive rate (y-axis) increases faster than the false positive rate (x-axis) as the probability threshold decreases.

So, the “ideal point” is the top left corner of the graph, that is a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that the larger the AUC the better the classifier.

The AUC metric varies between 0.50 (random classifier) and 1.00. Values above 0.80 is an indication of a good classifier.

In this section, we’ll show you how to compute and plot ROC curve in R for two-class and multiclass classification tasks. We’ll use the linear discriminant analysis to classify individuals into groups.

```{r}
library(pROC)
# compute roc
res.roc <- roc(observed.classes, pred.prob)
plot(res.roc, print.auc = T)
```

The gray diagonal line represents a classifier no better than random chance.

A highly performing classifier will have an ROC that rises steeply to the top-left corner, that is it will correctly identify lots of positives without mis classifying lots of negatives as positives.

In our example, the auc is 0.85, which is close to the max (which is 1). so our classifier can be considered as very good. A classifier that performs no better than chance is expected to have an AUC 0.5 when evaluated on an independent test set not used to train the model. 

If we want a classifier model with a specificity of at lest 60%, then the sensitivity is about 0.88%. The corresponding probability threshold can be extract as follow

```{r}
# extract some interesting results
roc.data <-data_frame(
  thresholds = res.roc$thresholds, 
  sensitivity = res.roc$sensitivities, 
  specificity = res.roc$specificities
)
# get the probability threshold for specificity = 0.6
roc.data %>% filter(specificity >= 0.6)
```

The best threshold with the highest sum sensitivity + specificity can be printed as follow. There might be more than one threshold.

```{r}
plot.roc(res.roc, print.auc = T, print.thres = "best")
```


# Section - 6 Articles - Statistical Machine Learning Essentials

Statistical Machine learning refers to a set of powerful automated algorithms that are used to predict an outcome variable based on multiple predictor variables. The algorithms automatically improve their performance through learning from the data, which means they are data driven and do not seek to impose linear or other overall structure on the data. the entire concept is referring to they are non - parametric 

The different machine learning methods can be used for both

Classification, where the outcome variable is a categorical variable
  True or False, positive or negative. 
Regression, where the outcome variable is a continuous variable

This section we will be covering the following methods:

K-Nearest Neighbors, which predict the outcome of a new observation x as the average outcome of the k most similar observations to x

Decision trees, which build a set of decision rules describing the relationship between predictors and the outcome. These rules are used to predict the outcome of a new observations 

Ensemble learning, including bagging, random forest and boosting. These machine learning algorithm are based on decision trees. They produce many tree models from the training data sets, and use the average as the predictive model. These results to the top-performing predictive modeling techniques. 

#KNN algorithm 

##KNN algorithm for classification

To classify a given new observation, the k nearest neighbors method starts by identifying the k most similar training observations to our new observations, and then assigns new observation to the class containing the majority of its neighbors


KNN algorithm for regression 

Similarly to predict a continuous outcome value for given new observation, the KNN algorithm computes the average outcome value of the k training observations that are the most similar to new observations, and returns this value as new observation predicted outcome value. 

Similarity measures

Note that, the dis similarity between observations is generally determind using Euclidean distance measure, which is very sensitive to the scale one which predictor variable measurements are made. So, it's generally recommended to standardize the predictor variables for making their scales comparable. 

The following sections hows how to build a k nearest neighbor predictive model for classification and regression settings. 

## load the data and libraries

```{r}
library(tidyverse)
library(caret)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
  createDataPartition(p = 0.8, list = F)
train.data <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```


## Compute KNN classifier

We’ll use the caret package, which automatically tests different possible values of k, then chooses the optimal k that minimizes the cross-validation (“cv”) error, and fits the final best KNN model that explains the best our data.

Additionally caret can automatically preprocess the data in order to normalize the predictor variables.

We’ll use the following arguments in the function train():

trControl, to set up 10-fold cross validation
preProcess, to normalize the data
tuneLength, to specify the number of possible k values to evaluate

```{r}
set.seed(123)
model <- train(
  diabetes ~., data = train.data, method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center", "scale"),
  tuneLength = 20
)
# plot model accuracy vs different values of k 
plot(model)
```

```{r}
model$bestTune
```

```{r}
# make prediction on the test data 
pred.classes <- model %>% predict(test.data)
#compute model accuracy rate 
mean(pred.classes == test.data$diabetes)
```

Now, we have an overall prediction accuracy of 76.92% which is pretty good. 

## KNN for regression

In this section, we will predict a continuous variable using KNN

we will use Boston data from MASS package for predicting the median house value (mdev), in Boston Suburbs, using different predictor variables. 

## prep the data

```{r}
data("Boston", package = "MASS")
set.seed(123)
training.samples <-Boston$medv %>% createDataPartition(p=0.8,list = F)
train.data <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
```

## Compute KNN using caret

The best K is the one that minimize the prediction error RMSE (root mean squard error)

The rmse corresponds to the square root of the average difference between the observed known outcome values and the predicted values, RMSE = mean((observed - predicted)^2) %>% sqrt(). The lower the rmse, the better the model 

```{r}
set.seed(123)
model <- train(
  medv~., data = train.data, method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center", "scale"), 
  tuneLength = 10
)
plot(model)
model$bestTune
pred <- model %>% predict(test.data)
head(pred)
RMSE(predictions, test.data$medv)
```

This chapter describes the basics of KNN (k-nearest neighbors) modeling, which is conceptually, one of the simpler machine learning method.

It’s recommended to standardize the data when performing the KNN analysis. We provided R codes to easily compute KNN predictive model and to assess the model performance on test data.

When fitting the KNN algorithm, the analyst needs to specify the number of neighbors (k) to be considered in the KNN algorithm for predicting the outcome of an observation. The choice of k considerably impacts the output of KNN. k = 1 corresponds to a highly flexible method resulting to a training error rate of 0 (overfitting), but the test error rate may be quite high.

You need to test multiple k-values to decide an optimal value for your data. This can be done automatically using the caret package, which chooses a value of k that minimize the cross-validation error


## CART Model: Decision Tree Essentials

Decision tree is very important and is a powerful / popular predictive machine learning technique that is used for classification and regression. So it is also known as classification and regression trees (CART).

To implement cart algorithm, we need to install RPART.

```{r}
library(tidyverse)
library(caret)
library(rpart)
```

## Decision Tree Algorithm
The algorithm of decision tree models works by repeatedly partitioning the data in to multiple sub spaces. So that the outcomes in each final sub space is as homogeneous as possible. This is technically called recursive partitioning.

2 possible results

continuous variable for regression trees
categorical variable for classification trees

```{r}
library(rpart)
model <- rpart(Species ~.,data = iris)
par(xpd = NA) #otherwise on some devices the test is clipped
plot(model)
text(model, digits =4)
```
The plot shows the different possible splitting rules that can be used effectively predict the type of outcome(here, iris species). for example, the top split assigns observations having Petal.length <2.45 to the left branch, where the predicted species are setosa. 

the different rules in tree can be printed as fllow:

```{r}
print(model, digits = 2)
```

If you have some sort of knoweldge in CS, it is actually almost the same thing with the tree algorithm you have seen in CS.

These rules are produced by repeatedly splitting the predictor variables, starting with the variable that has the highest association with the response variable. The process continues until some predetermined stopping criteria are met.

The resulting tree is composed of decision nodes, branches and leaf nodes. The tree is placed from upside to down, so the root is at the top and leaves indicating the outcome is put at the bottom.

Each decision node corresponds to a single input predictor variable and a split cutoff on that variable. The leaf nodes of the tree are the outcome variable which is used to make predictions.

The tree grows from the top (root), at each node the algorithm decides the best split cutoff that results to the greatest purity (or homogeneity) in each subpartition.

The tree will stop growing by the following three criteria (Zhang 2016):

all leaf nodes are pure with a single class;
a pre-specified minimum number of training observations that cannot be assigned to each leaf nodes with any splitting methods;
The number of observations in the leaf node reaches the pre-specified minimum one.
A fully grown tree will overfit the training data and the resulting model might not be performant for predicting the outcome of new test data. Techniques, such as pruning, are used to control this problem.

## Choosing the trees split points

Technically, for regression modeling, the split cutoff is defined so that the residual sum of squared error (RSS) is minimized across the training samples that fall within the subpartition.

Recall that, the RSS is the sum of the squared difference between the observed outcome values and the predicted ones, RSS = sum((Observeds - Predicteds)^2). See Chapter @ref(linear-regression)

In classification settings, the split point is defined so that the population in subpartitions are pure as much as possible. Two measures of purity are generally used, including the Gini index and the entropy (or information gain).

For a given subpartition, Gini = sum(p(1-p)) and entropy = -1* sum(p * log(p)), where p is the proportion of misclassified observations within the subpartition.

The sum is computed across the different categories or classes in the outcome variable. The Gini index and the entropy varie from 0 (greatest purity) to 1 (maximum degree of impurity)

Making predictions
```{r}
newdata <- data.frame(Sepal.Length = 6.5, Sepal.Width = 3.0,
                      Petal.Length = 5.2, Petal.Width = 2.0)
model %>% predict(newdata, "class")

```

## Classification trees

```{r}
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
sample_n(PimaIndiansDiabetes2, 3)
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```

## Fully Grown Trees
```{r}
set.seed(123)
model1 <-rpart(diabetes~., data = train.data, method = "class")
par(xpd = NA)
plot(model1)
text(model1, digits = 3)
```

make predictions on the test data 
```{r}
predicted.classes <-model1 %>% predict(test.data, type = "class")
head(predicted.classes)
mean(predicted.classes == test.data$diabetes)
```

However, this full tree including all predictor appears to be very complex and can be difficult to interpret in the situation where you have a large data sets with multiple predictors.

Additionally, it is easy to see that, a fully grown tree will overfit the training data and might lead to poor test set performance.

A strategy to limit this overfitting is to prune back the tree resulting to a simpler tree with fewer splits and better interpretation at the cost of a little bias (James et al. 2014, P. Bruce and Bruce (2017)).

## Pruning the tree
Briefly, our goal here is to see if a smaller subtree can give us comparable results to the fully grown tree. If yes, we should go for the simpler tree because it reduces the likelihood of overfitting.

One possible robust strategy of pruning the tree (or stopping the tree to grow) consists of avoiding splitting a partition if the split does not significantly improves the overall quality of the model.

In rpart package, this is controlled by the complexity parameter (cp), which imposes a penalty to the tree for having two many splits. The default value is 0.01. The higher the cp, the smaller the tree.

A too small value of cp leads to overfitting and a too large cp value will result to a too small tree. Both cases decrease the predictive performance of the model.

An optimal cp value can be estimated by testing different cp values and using cross-validation approaches to determine the corresponding prediction accuracy of the model. The best cp is then defined as the one that maximize the cross-validation accuracy (Chapter @ref(cross-validation)).

Pruning can be easily performed in the caret package workflow, which invokes the rpart method for automatically testing different possible values of cp, then choose the optimal cp that maximize the cross-validation (“cv”) accuracy, and fit the final best CART model that explains the best our data.

You can use the following arguments in the function train() [from caret package]:

trControl, to set up 10-fold cross validation
tuneLength, to specify the number of possible cp values to evaluate. Default value is 3, here we’ll use 10.

```{r}
set.seed(123)
model2 <- train(
  diabetes ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
# plot model accuracy vs different values of cp (complexity parameter)
plot(model2)
```
Get the best tune parameter cp that maximizes the model accuracy
```{r}
model2$bestTune
```

```{r}
par(xpd = NA)
plot(model2$finalModel)
text(model2$finalModel, digits =3)
```

```{r}
model2$finalModel
predicted.classes <- model2 %>% predict(test.data)
mean(predicted.classes == test.data$diabetes)
```

From the output above, it can be seen that the best value for the complexity parameter (cp) is 0.2735043, allowing a simpler tree, easy to interpret, with an overall accuracy of 0.7307692, which is comparable to the accuracy (0.7564103) that we have obtained with the full tree. The prediction accuracy of the pruned tree is even better compared to the full tree.

Taken together, we should go for this simpler model.

##Regression trees

Previously, we described how to build a classification tree for predicting the group (i.e. class) of observations. In this section, we’ll describe how to build a tree for predicting a continuous variable, a method called regression analysis 

The R code is identical to what we have seen in previous sections. Pruning should be also applied here to limit overfiting.

Similarly to classification trees, the following R code uses the caret package to build regression trees and to predict the output of a new test data set.

```{r}
# Load the data
data("Boston", package = "MASS")
# Inspect the data
sample_n(Boston, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
# Fit the model on the training set
set.seed(123)
model <- train(
  medv ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Plot model error vs different values of
# cp (complexity parameter)
plot(model)
# Print the best tuning parameter cp that
# minimize the model RMSE
model$bestTune
```


Plot the final model
```{r}
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model$finalModel)
text(model$finalModel, digits = 3)
```

```{r}
# Decision rules in the model
model$finalModel
# Make predictions on the test data
predictions <- model %>% predict(test.data)
head(predictions)
# Compute the prediction error RMSE
RMSE(predictions, test.data$medv)
```

Conditional inference tree

The conditional inference tree (ctree) uses significance test methods to select and split recursively the most related predictor variables to the outcome. This can limit overfitting compared to the classical rpart algorithm.

At each splitting step, the algorithm stops if there is no dependence between predictor variables and the outcome variable. Otherwise the variable that is the most associated to the outcome is selected for splitting.

```{r}
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
# Split the data into training and test set
set.seed(123)
training.samples <- pima.data$diabetes %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- pima.data[training.samples, ]
test.data <- pima.data[-training.samples, ]
```

Build conditional trees using the tuning parameters maxdepth and mincriterion for controlling the tree size. caret package selects automatically the optimal tuning values for your data, but here we’ll specify maxdepth and mincriterion.
The following example create a classification tree:
```{r}
library(party)
set.seed(123)
model <- train(
  diabetes ~., data = train.data, method = "ctree2",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(maxdepth = 3, mincriterion = 0.95 )
  )
plot(model$finalModel)
# Make predictions on the test data
predicted.classes <- model %>% predict(test.data)
# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)
```


The p-value indicates the association between a given predictor variable and the outcome variable. For example, the first decision node at the top shows that glucose is the variable that is most strongly associated with diabetes with a p value < 0.001, and thus is selected as the first node.


This chapter describes how to build classification and regression tree in R. Trees provide a visual tool that are very easy to interpret and to explain to people.

Tree models might be very performant compared to the linear regression model (Chapter @ref(linear-regression)), when there is a highly non-linear and complex relationships between the outcome variable and the predictors.

However, building only one single tree from a training data set might results to a less performant predictive model. A single tree is unstable and the structure might be altered by small changes in the training data.

For example, the exact split point of a given predictor variable and the predictor to be selected at each step of the algorithm are strongly dependent on the training data set. Using a slightly different training data may alter the first variable to split in, and the structure of the tree can be completely modified.

Other machine learning algorithms - including bagging, random forest and boosting - can be used to build multiple different trees from one single data set leading to a better predictive performance. But, with these methods the interpretability observed for a single tree is lost. Note that all these above mentioned strategies are based on the CART algorithm. 









