---
title: "ML Basic"
author: "JEFFREY CHAN"
date: "12/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Reference website

http://www.sthda.com/english/articles/40-regression-analysis/165-linear-regression-essentials-in-r/#multiple-linear-regression

Linear Regression Essentials in R

load / install the requirement packages

```{r}
library(tidyverse)
library(caret)
theme_set(theme_bw())
```

Preparing the data

sample_n(data, col)

```{r}
data("marketing", package = "datarium")
sample_n(marketing,3)
```

```{r}
# split the data into training and test set
set.seed(123)

# We’ll randomly split the data into training set (80% for building a
# predictive model) and test set (20% for evaluating the model).

training.samples <- marketing$sales %>%
  createDataPartition(p=0.8, list = F)
train.data <- marketing[training.samples, ]
test.data <- marketing[-training.samples, ]

# build model 
model <- lm(sales ~., data = train.data)

# summarize the model
summary(model)

summary(model)$coefficient

# make prediction 
predictions <- model %>% predict(test.data)

# model performance 
# (a) Prediction error, RMSE
RMSE(predictions, test.data$sales)

# (b) R-square
R2(predictions, test.data$sales)
```

Simple Linear Regression 

```{r}
model_you <- lm(sales ~ youtube, data = train.data)
summary(model_you)$coef
```

Make prediction 
```{r}
newdata <- data.frame(youtube = c(0,1000))
model_you %>% predict(newdata)
```

Multiple Linear Regression 

```{r}
model3 <- lm(sales ~ youtube + facebook + newspaper, data = train.data)
summary(model3)$coef
```

Quick note, i have a lot of predictor, we can simply use ~. to include all the predictor

```{r}
model3_1 <- lm(sales ~., data = train.data)
summary(model3_1)$coef
```

Col1:  b0 / y intercept is 3.73546064
Col2: std.error = 0.44062. this represent the accuracy of the coefficients.  we always want small value for std.error
Col3: T value is the t statistics estimate / std error = t value
Col4: P value for the T statistics. The smaller the p value the more significant the estimate is. 

Let's make prediction for the values 
```{r}
# New advertising budget
newdata <- data.frame( youtube = 2000, facebook = 1000, newspaper = 1000)

# predict sales values
model3_1 %>% predict(newdata)
```

Interpretation.

Before using the model for predictions, i need to access the statsitical significance of the model. We need to apply summary(model_name)
```{r}
summary(model)
```

The residuals, should be normally distributed. Looks like our data is kind of normally distributed. by theory, mean should be zero. Q1 and Q3, min and max should be around the same with + or - sign. 

coefficients, shows the paramete values and their significance. 
If they are significant, it will be shown with starts

RSE / R^2 / F statistics are used to check how well the model fits to our data

First step, always check the F statistics and the associated p value and the bottom of the model summary

Coefficients significance
```{r}
summary(model)$coef
```

Important, from the summary table , we can see that youtube, facebook advertising budget are significantly changing the sales while by newpaper, there is not much of a change. 

Also, to interpret the intercept, we can say every 1000 dollar i invest in facebook advertising i will have a return of 1000* 0.19398450 = 193 sale unit. So do youtube, 1000* 0.04516611 = 45.16 sale units 

Since newspaper does not affect the outcome much, lets remove it from the model 
```{r}
model4 <- lm(sales ~ youtube + facebook, data = train.data)
summary(model4)
```
now our equation can be written as 
sales = 3.577663 + 0.045287(youtube) + 0.190299(facebook)

Model accuracy

Well, after knowing it is significant, we would like to know how well the model fits our data. 
This process will be referred to as the goodness of fit
The overall quality of the linear regression fit can be shown by model summary
RSE, R^2, adjusted R^2, F statistics 

RSE / model sigma -> prediction error. it is the observed out come - predicted value (Y_i - Y bar). small rse is the best the model fits to the data.
dividing the RSE by the average value of the outcome variable will give me the prediction error rate. Which should be as small as possible. 

in our example, we have 1.853 for RSE. 
```{r}
(1.853/(mean(train.data$sales)))*100
```
It is 10.90% which is low. 

R-squared and adjusted R squared
R^2 is ranges between 0 and 1. Higher the R square the better the model. High R square = observed data is very close to the prediction data. So the quality of the regression line is pretty good.R^2 = pearson ^2. Here is the trick tho, if i have a lot of parameters, R^2 will increase along with it.

However, the adjusted R^2 is the correction of the for the number of parameters in the predictive model.Therefore, I should always read adjusted R^2 because it will make the correction according the to incorrect R^2
When adjusted R^2 = 0 thats mean the mdoel did not explain much about the variability in the outcome. 
In our outcome, adjusted R^2 is 0.9112 so it is pretty good. 

Lastly, F statistics gives the overall significance of the model. it tells us whether at least one predictor variables has non zero coefficient. 
In a simple linear regression ( one parameter), it wont be interesting because it is just a duplicated info given by the t test from the coef table.

The F statistic becomes more important once we start using multiple predicators as in multiple linear regression.

So according to what we have, our F statistics equal 825.4 with 2 parameters and 159 df, with a p-value of 2.2e^-16. which is highly significant. In order to read the p value, p < 0.05 will be significant. 

Making predictions

Procedure to make predictions
1) predict the sales values based on new advertising budgets in the test data
2) Assess the model performance by computing:
  the prediction error RMSE (Root Mean Squared Error), representing the average difference between the observed known outcome values in the test data and the preidcted outcome values by the model. The lower the RMSE, the better the model. 
   The R^2, representing the correlation between the observed outcome values and the predicted outcome values. the higher the r^2, the better the model. 
   
```{r}
# make predictions 
predictions <- model %>% predict(test.data)

# model performance 
# (a) compute the prediction error, RMSE
RMSE(predictions, test.data$sales)
```
```{r}
R2(predictions, test.data$sales)
```

```{r}
(RMSE(predictions, test.data$sales)/ mean(test.data$sales))*100
```
The % error is 16.39% is alright

Discussion

This section discuss basic linear regression and provides practical examples in R for computing simple and multiple linear regression model. Also, we have learnt how the accuracy of the model. 
The idea of linear regression is to see the predictor relationship with the response. 
It can be easily be visulaized by a basic plot without working a lot on lm() summary and more. 

```{r}
ggplot(marketing, aes(youtube, sales)) + 
  geom_point() +
  stat_smooth()
```

As we can expected, since we know that the Beta_1 is positive from what we have done before, we can expect that the slope is positive and the scatter dots are following the positve slope.

http://www.sthda.com/english/articles/40-regression-analysis/164-interaction-effect-in-multiple-regression-essentials/

Interaction Effect in Multiple Regression: Essentials

Interaction effects


```{r}
# build the model
# use this:
model12 <- lm(sales ~ youtube*facebook, data = train.data)
summary(model12)
```

Make predictions 

```{r}
predictions12 <- model12 %>% predict(test.data)
# model performance
# (a) prediction error, RMSE
RMSE(predictions12, test.data$sales)
# (b) R-square
R2(predictions12, test.data$sales)
# % error
(RMSE(predictions12, test.data$sales) / mean(test.data$sales))*100
```

Those values are pretty good. high R^2 and 10.67% error

```{r}
summary(model12)$coef
```
As you can see that all of them are significant (Pr(>|t|)). so it means there is an interaction relationship between the two predictor variables(youtube and facebook advertising)

our model will be like 
sales = 7.89 + 0.0189949052(youtube) + 0.0330506939(facebook) + 0.0008751096 (youtube*facebook)

comparing the additive and the interaction models. 

The prediction error RMESE of the interaction model is 1.721177 compared with the prediction error of the addictive model  2.642146, interaction model is lower.

R^2 of the interaction model is 0.9373706, and for the addictive model has 0.8322611.
Interaction model has a better R^2

Lastly, these result suggest that the model with the int eraction term is better than the model that contains only main effects. 
So for this specific data, we should go for the model with the interactio model.

Discussion, after finding addictive model which is significant, we should also check if the interaction model is also significant. If they do, we should adapt the interaction model. 



http://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/

Regression with Categorical Variables: Dummy Coding Essentials in R

```{r}
library(car)
# load data
data("Salaries", package = "car")
# Inspect the data
sample_n(Salaries, 3)
```

Categorical variables with two levels
From our data set, we would like to investigate differences i nsalaries between males and females. 
Based on the gender, we can say m = 1, female = 0
b0 + b1 if person is male
bo if person is female.
B0 is average salary among females
B0 + B1 = average salary among males
B1 is average difference in salary between males and females

```{r}
mwage <- lm(salary ~ sex, data = Salaries)
summary(mwage)$coef
```

From the above result, average salary for female is 101002.41. Male = B0 + B1 = 101002.41 + 14088.01 = 115090.40
the P value for both sex is very significant, which is suggesting that there is a statistical evidence of a difference in average salary between the genders.

```{r}
contrasts(Salaries$sex)
```

I can use the function relevel() to set the baseline category to males as follow
```{r}
Salaries <- Salaries %>%
  mutate(sex = relevel(sex, ref = "Male"))
```

```{r}
mwage1 <- lm(salary ~ sex , data = Salaries)
summary(mwage1)$coef
```

Categorical varaibles with more than two levels

Generally, categorical variable with n levels will be tranformed in to n-1 variables each with two levels. These n-1 new varaibles contain the same info than the single variable. This recording creates a table called contrast matrix.

In salaries, data has three levels, asstprof, assocprof, and prof. These variables could be dummy coded in to two variables, one called assocprof and one prof. 

if rank = assocprof, then the column assocprof would be coded with 1 and prof with 0
if rank = prof, then the col assocprof would be coded with a 0 and prof would be coded with a 1
if rank = asstprof then both cols assocprof and prof would be coded with a 0

```{r}
res <- model.matrix(~rank, data=Salaries)
head(res[,-1])
head(res)
```
R will always use the first level as a reference and interpret the remaining levels relative to the first level

The following code will give you the level
```{r}
levels(Salaries$rank)
```
Indeed, AsstProf will be the reference level.

Also ANOVA(analyse of variance) is just a special case of linear model where the predictors are categorical variables. R understands the fact that ANOVA and regression are both examples of linear models, it lets you extract the classic ANOVA table from the regressio nmodel using the R base anova() function or the Anova() function. We generally use Anova() function because it automatically takes care of unbalanced designs.

Lets predict the salary from using a multiple regression procedure
```{r}
mwage2 <- lm(salary ~ yrs.service + rank + discipline + sex, data = Salaries)
Anova(mwage2)
```
```{r}
Anova(mwage1)
```
When we take rank, discipline, yrs of service in to consideration, the variable sex is no longer significant combined with the variation in salary between individuals.

```{r}
summary(mwage2)
```

```{r}
levels(Salaries$discipline)
```
apply ??salaries and check discipline a and b is corresponding to what field
from the documentation, dis A is theoretical, dis B is applied department. 

For example, from dicipline B (applied departments) is significantly associated with an average increase of 13473.38 in salary compared (there is a difference) to discipline theoretical departments.
So that is the reason why discipline B is significant


Nonlinear Regression Essentials in R: Polynomial and Spline Regression Models

http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/

In this section, you’ll learn how to compute non-linear regression models and how to compare the different models in order to choose the one that fits the best your data.

Will also be using RMSE and R2 metric to compare the different models' accuracy
Recall, RMSE -> model prediction error. Thats the average difference of the observed outcome values and predicted outcome values.

R2 represents the squared correlation. How accurate is the data, if it is exactly on the regression line, that the R^2 will be 1

The best model is the model with the lowest RMSE and the highest R2

```{r}
library(tidyverse)
library(caret)
theme_set(theme_classic())
```

Prepare the data 

we will use the boston data from MASS package
```{r}
# Load the data, if the package is not installed, instal it now and load the library
if(!require("MASS")){
  install.packages("MASS")
  library(MASS)
}
data("Boston", package = "MASS")
str(Boston)
# Split the data in to training and test set
set.seed(123)
ts <- Boston$medv %>%
  createDataPartition(p =0.8, list = F)
trd <- Boston[ts, ]
ted <- Boston[-ts, ]
```
Visualize the scatter plot of the medv vs lstat varaibles, both medv and lstat is from the Boston dataset

use stat_smooth when i want to display the results with non standard geom
```{r}
ggplot(trd, aes(lstat,medv)) + 
  geom_point() +
  stat_smooth()
```

It is obvious to see that the blue line is a curve


Linear Regression

```{r}
# Build the model
modelc <- lm(medv ~ lstat, data = trd)
# make prediction
pmodelc <- modelc %>% predict(ted)
data.frame(
  rmseL <- RMSE(pmodelc, ted$medv),
  r2L <- R2(pmodelc, ted$medv),
  perrorL <- rmseL / mean(ted$medv)
)
```
I have a pretty high percent error 26.82%

visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = lm, formula = y ~ x)
```

Polynomial Regression

Lets make it polynomial regression
medv is the response.

we should have this following formula medv = b0 + b1 * lstat + b2*lstat^2
in r to make that ^2 we need to apply I(x^2)

```{r}
modelc1 <- lm(medv ~ lstat + I(lstat^2), data = trd)
summary(modelc1)
```

Alternative way to create 2 degree regression model

```{r}
modelc2<- lm(medv ~ poly(lstat, 2, raw =T), data = trd)
summary(modelc2)
modelc2p <- modelc2 %>% predict(ted)
p2 <- c(RMSE(modelc2p,ted$medv), R2(modelc2p, ted$medv), RMSE(modelc2p, ted$medv) / mean(ted$medv))
```

As you can see they are the same. intercepts and the coefficient of beta1 and beta2^2 are all significant. as well as the F statistic P value is also small. Indeed, we have an Adjusted R^2 0.6329 which is ok. 

The following is the 6th order
```{r}
modelc6 <- lm(medv ~ poly(lstat, 6, raw = T), data = trd)
summary(modelc6)
modelc6p <- modelc6 %>% predict(ted)
p6 <- c(RMSE(modelc6p,ted$medv), R2(modelc6p, ted$medv), RMSE(modelc6p, ted$medv) / mean(ted$medv))
```
From this point we can see that after degree 3 it is no longer significant. 
Then we will just simply use degree 3 for our model

```{r}
modelc5 <- lm(medv ~ poly(lstat, 5, raw = T), data = trd)
summary(modelc5)
```
```{r}
modelc5p <- modelc5 %>% predict(ted)
p5 <- c(RMSE(modelc5p,ted$medv), R2(modelc5p, ted$medv), RMSE(modelc5p, ted$medv) / mean(ted$medv))
modelc3 <- lm(medv ~ poly(lstat, 3, raw = T), data = trd)
modelc3p <- modelc3 %>% predict(ted)
p3 <- c(RMSE(modelc3p,ted$medv), R2(modelc3p, ted$medv), RMSE(modelc3p, ted$medv) / mean(ted$medv))
modelc4 <- lm(medv ~ poly(lstat, 4, raw = T), data = trd)
modelc4p <- modelc4 %>% predict(ted)
p4 <- c(RMSE(modelc4p,ted$medv), R2(modelc4p, ted$medv), RMSE(modelc4p, ted$medv) / mean(ted$medv))
perrorp4 <- RMSE(modelc4p, ted$medv) / mean(ted$medv)
modelc7 <- lm(medv ~ poly(lstat, 7, raw = T), data = trd)
modelc7p <- modelc7 %>% predict(ted)
p7 <- c(RMSE(modelc7p,ted$medv), R2(modelc7p, ted$medv), RMSE(modelc7p, ted$medv) / mean(ted$medv))
```
Lets see the RMSE, R2 and percent error p2, p3,p4, p5, p6
```{r}
temp <- cbind(p2,p3,p4,p5,p6,p7)
row.names(temp) <-c("RMSE", "R2", "% error")
temp
```
There are couple patterns are going on. First im looking at the table above, we can see that p5 is the dip of the % error. after that it bounces back and has a small changes at p7. Also, if we look at the anova(p1 : p7) the significance level stops at 4. 
```{r}
anova(modelc2,modelc3,modelc4,modelc5, modelc6, modelc7)
```
So i would say degree 4 is the best option.
```{r}
modelc4 <- lm(medv ~ poly(lstat, 4, raw = T), data = trd)
modelc4p <- modelc4 %>% predict(ted)
p4 <- c(RMSE(modelc4p,ted$medv), R2(modelc4p, ted$medv), RMSE(modelc4p, ted$medv) / mean(ted$medv))
p4
```
```{r}
summary(modelc4)
```
The resudials distribution is alright. By theory, the median should be 0 and 1Q, 3Q, and min max should be balanced. so this is not the best model but thats all we have. 

Let's visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ poly(x, 4, raw = T))
```

Log Transformation
When i have a non linear relationship, i can also try a log transformation of the predictor variables. 

```{r}
# build the model 
modelcLog <- lm(medv ~ log(lstat), data = trd)

# make predictions
pmodelcLog <- modelcLog %>% predict(ted)

# model performance 
(rmseLog <- RMSE(pmodelcLog, ted$medv))
(r2Log <-  R2(pmodelcLog, ted$medv))
(perrorLog <- rmseLog/ mean(ted$medv))
```

visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ log(x))
```

Spline regression
```{r}
library(splines)
# splines becomes a base package so you shouldnt be install 
# it if you are using R version 4.0.2
knots <- quantile(trd$lstat, p = c(.25,.5,.75))
# build model 
modelcSp <- lm(medv ~ bs(lstat, knots = knots), data = trd)
# make prediction
pmodelcSp <- modelcSp %>% predict(ted)

# model performance
(rmseSp <- RMSE(pmodelcSp, ted$medv))
(r2Sp <-  R2(pmodelcSp, ted$medv))
(perrorSp <- rmseSp/ mean(ted$medv))
```

Lets visualize the data
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df =4))
```

Generalized additive models

Here is another problem. Once I know that the model is non linear relationship in my data, the polynomial terms might not be flixible enough to capture the relationship, and the spline terms require speccifing the knots. 
Therefore, we have Generalized additive models, GAM, are a technique to automatically fit a spline regression. We need mgcv package
```{r}
library(mgcv)
# build the model
modelcGam <- gam(medv ~s(lstat), data = trd)
# make predictions
pmodelcGam <- modelcGam %>% predict(ted)
# Model performance
data.frame(
  rmsegam <- RMSE(pmodelcGam, ted$medv),
  r2gam <- R2(pmodelcGam, ted$medv),
  perrorGam <- rmsegam / mean(ted$medv))
```

Visualize the plot
```{r}
ggplot(trd, aes(lstat, medv)) + 
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
```
Lets compare all the models linear, p^4, log, splines, and GAM
```{r}
(fourmodelsPercentError <- rbind(perrorL, perrorp4, perrorLog, perrorSp, perrorGam))
```
Among all 5 models, the best model we have is splines. why? think of how it works. 
it's like integral, it breaks in to small little part and find the slope thats why the stat_smooth fits the best of the data. Therefore Sp is the best model 


Last section.

Multiple Linear Regression in R

http://www.sthda.com/english/articles/40-regression-analysis/168-multiple-linear-regression-in-r/

A little bit duplicated with the first section. However, it will have a slighly more indepth about the performance and accuracy of a model 

```{r}
data("marketing", package = "datarium")
modelLS <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(modelLS)

```

Let's interpret the data

First we might want to look at the F statistics' p value
We have a pretty high significant value. This mean that, at least one of the variable is significantly related to the outcome. 
You can think of it is true that the parameter ( youtube / facebook / newspaper) has the relationship with the response variable 

You should have this question, so which one has the relationship with the response varaible?

let's take a look with the following code
```{r}
summary(modelLS)$coef
```

Look at the T statistics is checking if the b0, b1, b2, b3 is 0 or not. look at the newspaper, we have -0.001037493 as the beta3 right? and look at the t value, -0.1767146. 
the T value for the newspaper is pretty close to 0 and look at B3, we have -0.001037493, it simply no use at all so we can simply create another model without the news paper. 
but the rest, youtube, facebook have a high impact for the sales. There is a relationship with the sales. So more money to put in to youtube and facebook, we have more feedback from the sales. 

Lets create another model 

```{r}
modelLSyf <- lm(sales ~ youtube + facebook, data = marketing)
summary(modelLSyf)
```

95% confident interval for the coefficient

```{r}
confint(modelLSyf)
```

Model accuracy assessment

As we know, simple linear regression, the overall quality is based on R^2 and Residual standard error (RSE)

Remember one thing, the more parameters we have the higher the R^2. eventhough the parameter does not have much effect on the Y response, it will still in play with the R square.  Thats the reason why we have adjusted R^2

Residual standard error (RSE), or sigma

The RSE estimate gives a measure of error of prediction. we want small RSE
```{r}
sigma(modelLSyf)/mean(marketing$sales)
```
This number is not to bad. so we have approximately 12% error rate. 

Some cool trick, we dont have to type all the parameters. we can simply do 

model <- lm(sales ~. data = marketing) 
that ~ means all 

now if we would like to drop the newspaper from the parameter list
model <- lm(sales ~. - newspaper, data = marketing)

Alternative 
model <- update(model, ~. - newspaper)


Predict in R: Model Predictions and Confidence Intervals

http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/

Outcome: predict outcome for new observations data, display confidence intervals and the  predictio intervals. 

```{r}
# load the data 
data("cars", package = "datasets")
# build the model 
modelcar <- lm(dist ~ speed, data = cars)
modelcar 
```

Prediction for new data set

```{r}
# create data 
cardata <- data.frame(speed <- c(12,19,24))
# predict data
predict(modelcar, newdata = cardata)
```

Confidence interval 

by default the confidence interval is 95%

```{r}
predict(modelcar, newdata = cardata, interval = "confidence")
```

From our newly created data. Lets take 19 miles per hr as our example. 
Distance is in ft
we can say that 19 miles per hr has an average stopping dist between 51.82913 to 62.44421. the regression line has predict the value for 19mph with a stopping dist @ 57.13667

Prediction interval
prediction interval gives an uncertainty around a single value. 

```{r}
predict(modelcar, newdata = cardata, interval = "prediction")
```

From the table above, we can say that 95% prediction intervals tell us that with a speed of 19 mph with the stopping distance is 25.76 to 88.51. 
This means that, based on our model 95% of the cars with a speed of 19 mph have a stopping distance between 25.76 and 88.51

Now you should have at least one question in your mind at this point.
WTH? what is the difference between prediction interval and confidence interval?

The key word for both interpreting is average. 

prediction interval is predicting a single future value at that point. 

confidence interval is predicting the mean. 

https://stats.stackexchange.com/questions/16493/difference-between-confidence-intervals-and-prediction-intervals

More on prediction interval or confidence interval 

A prediction interval reflects the uncertainty around a single value, while a confidence interval reflects the uncertainty around the mean prediction values. Thus, a prediction interval will be generally much wider than a confidence interval for the same value.

Generally, we are interested in specific individual predictions. So a prediction interval would be more appropriate. Using a confidence interval when you should be using a prediction interval will be underestimate the uncertainty in a given predicted value 

lets build the prediction band and confidence inteval

```{r}
# build the model
data("cars", package = "datasets")
modelcar1 <- lm(dist~speed, data =cars)
# add the predictions
p.int <- predict(modelcar1, interval = "prediction")
carD <- cbind(cars, p.int)
# Regression line + confidence intervals
p <- ggplot(carD, aes(speed, dist)) + 
  geom_point() + 
  stat_smooth(method = lm) + 
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") + #prediction interval
  geom_line(aes(y = upr), color = "red", linetype = "dashed") # prediction interval 
p
```

Regression Model Diagnostics 

Linear Regression Assumptions and Diagnostics in R: Essentials

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/


By the title, you can tell we are going to check if the model works well for the data.

1) inspect the significance of the regression beta coefficients
2) R^2

Those are what we have learnt from the previous sections

This section, we will learn the additional steps to evaluate how well the model fits the data.

For example, linear regression model makes the assumption that the relationship between x and y are linear. but that might not be true. The relationship could be polynomial or logarithmic 

the data might contain outlines which will affect the result. 

Thats why we need to diagnose the regression model and detect potential problems and check the assumptions is met by the linear regression model 

in order to do so, we will check the distribution of residuals errors, which will tell us more about my data. 

Main idea for this section. 
 - explaining residuals errors and fitted values. 
 - present linear regression assumptions, as well as potential problems you can will tackles while performing regression analysis
 - We will talk about some built in diagnostic plots in R to test the assumptions about our linear regression model. 
 
Lets get started 
 
```{r}
if(!require("broom")){
  install.packages("broom")
  library(tidyverse)
library(broom)
}
theme_set(theme_classic())
```


```{r}
# load the data 
data("marketing", package = "datarium")
# inspect the data
sample_n(marketing, 3)
```


Build the regression model 

```{r}
modelm <- lm(sales ~ youtube, data = marketing)
modelm
```

Fitted value ( predicted value) is the value that is predicted from the regression line. 
```{r}
ggplot(marketing, aes(youtube, sales)) +
  geom_point() +
  stat_smooth(method = lm)
```

augment is from broom package
```{r}
model.diag.metrics <- augment(modelm)
head(model.diag.metrics)
```
Lets plot the residuals error in red
```{r}
ggplot(model.diag.metrics, aes(youtube, sales)) + 
  geom_point() + 
  stat_smooth(method = lm, se = F) +
  geom_segment(aes(xend = youtube, yend = .fitted), color = "red", size =0.3)
```

In order to check the regression assumption, we need to see the distribution of the residuals

Linear regression makes several assumptions about the data, such as :

Linearity of the data. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.

Normality of residuals. The residual errors are assumed to be normally distributed.

Homogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity)

Independence of residuals error terms.


You should check whether or not these assumptions hold true. Potential problems include:

Non-linearity of the outcome - predictor relationships
Heteroscedasticity: Non-constant variance of error terms.
Presence of influential values in the data that can be:
  Outliers: extreme values in the outcome (y) variable
  High-leverage points: extreme values in the predictors (x) variable

Those problems can be solved by diagnostic plot

```{r}
par(mfrow = c(2,2))
plot(modelm)
```
How to read it?

https://data.library.virginia.edu/diagnostic-plots/

Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.

Normal Q-Q. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line.

Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity problem.

Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. This plot will be described further in the next sections.

Lets create a better table for model.diag.metrics
```{r}
names(model.diag.metrics)
```
Before and after

```{r}
index <- c(1:nrow(model.diag.metrics))
model.diag.metrics1 <- data.frame(index,model.diag.metrics[ , c(-7)])
names(model.diag.metrics1)
```

fitted values = predicted value from the regression line
.resid = residual errors
.hat = outliers
.std.resid = detect outliers, extreme values -> standardized residuals = residuals / standard errors
.cooksd = Cook's distance -> detect influential values outliers or high leverage point $


Let's look at the plot one by one
```{r}
plot(modelm,1)
```

Good plot would be show no fitted pattern, which means the red line should be a horizontal line at 0. If the red line is not at 0 or around 0 there might be a problem with our linear model, which means we need to use non linear model. 
In our example, plot 1 residuals vs fitted, there is no pattern in the residual plot, so we can assume it is a linear relationship sales ~ youtube.

if the residual plot is non linear relationship, you need to transfrom your data ith log(x), sqrt(x), x^2 in the regression and so on. 


You might ask how do you know if it is non linear? well it will be obvious that red line is something else like quadratic or something. 
reference link: non linear 
https://i0.wp.com/blogs.sas.com/content/iml/files/2019/06/residsmooth1.png?ssl=1


Homogeneity of variance

This assumption can be checked by examining the scale - location plot, also known as the spread- location plot
```{r}
plot(modelm,3)
```
Good plot for scale - location is the data set is close to gether and close the the line. 
since this plot is spreading wider when x increases, so this is not a good plot. 

So i can say not a constant variances in the residuals error (heterosedasticity)

Remember, by theory epsilon has mean zero and the variance is constant.

a possible solution to reduce the variance is to use log or square root transformation for variable y


lets try again by transforming the plot
```{r}
modelmLog <- lm(log(sales) ~ youtube, data = marketing)
plot(modelmLog,3)
```
This plot looks a lot better

Normality of Residuals
this is also called QQ plot. This plot can check the normality assumption. The normal probability plot of residuals should approximately follow a straight line like below. 

```{r}
plot(modelm, 2)
```


Outliers and high Leverage points

The outlier will directly affect the RSE because it is so far away from the regression line. 
Outlier can be identified by examining the standardized residual ( or studentized residual), which is the residual divided by the estimated standard error. 

Observations whose standardized residuals are greater than 3 in absolute value are possible outliers (James et al. 2014).

High leverage points (hat value)

A value of this statistic above 2(p + 1)/n indicates an observation with high leverage (P. Bruce and Bruce 2017); where, p is the number of predictors and n is the number of observations.

Residuals vs Leverage plot
```{r}
plot(modelm, 5)
```
The plot will highlight the most extreme points. 26, 179, 36. as you can see no outliers that have exceed 3 standard deviations, which is a good plot. 


Influential values 

Not all outliers (extreme data points) are influential in linear regression analysis

We use Cook's distance to determine the influence of a value. This metric defines influence as a combination of leverage and residual size. 

High influence if Cook's distance exceeds 4/ (n - p - 1) n = # of observations  p = number of predictor variables.

again residuals vs leverage plot can help us to find influential observations. outlying values are generally located at the upper right corner or lower right corner. 

Those corners will have direct influential against a regression line. 

```{r}
par(mfrow=c(1,2))
# Cook's distance
plot(modelm, 4)
# Residuals vs Leverage
plot(modelm, 5)
```
Now we have been talking about Cook's distance for the last 10 mins, so how do i know if the outliers are influencing the regression line?

First from the Residuals vs Leverage plot, i am not seeing any red dashed line, so that means my outliers is not affecting the regression line a lot. If we see the dashed line thats mean we are close to the Cook's distance line which means it is some how affecting the regression line seriously. but usually if the outliers are within the Cook's distance we are all good. 

By default, only top 3 most extreme values are labeled on Cook's distance plot, if I want to add more i can use the following code
plot(modelm, 4, id.n = 5)

if i would like to access those distance later, i can use the following code
model.diag.metrics %?%
  top_n(3, wt = .cooksd)

Lets create something that is outside the Cook's distance

```{r}
dfcook <- data.frame(
  x <- c(marketing$youtube, 500,600),
  y <- c(marketing$sales, 80,100)
)
modelm2 <- lm(y~x, dfcook)
par(mfrow = c(1,2))
# Cook's distance
plot(modelm2, 4)
# Residuals vs Leverage
plot(modelm2, 5)
```
As you can see from the Residuals vs Leverage, you can see there are 2 outliers are outside of the dashed line (Cook's distance) those are the outliers that are affecting the regression line directly. In the other words, those data are outside of the cook's distance would simply mean they have a high cook's value.

The plot identified the influential observation as #201 and #202. If you exclude these points from the analysis, the slope coefficient changes from 0.06 to 0.04 and R2 from 0.5 to 0.6. Pretty big impact!

There are a lot of concepts that must be undersood, so take your time to go thru it again. 

Discussion. 
This section describes linear regression assumptions and how to diagnose the potential problems in the model
You must visualizing the residuals and the patterns in residuals is not a stop signal. your current regression model might not be the best way to udnerstand your data. 

Here are the potential problems:
non - linear relationships between the outcome and the predictor variables. When facing to this problem, one solution is to include a quadratic term, such as polynomial terms or log transformation. See Chapter 

Existence of important variables that you left out from your model. Other variables you didn’t include (e.g., age or gender) may play an important role in your model and data.

Presence of outliers. If you believe that an outliers have occurred due to an error in data collection and entry, then one solution is to simply remove the concerned observation.




